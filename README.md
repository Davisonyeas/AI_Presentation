# Explainable AI, Large Models, Neural Networks, and Beyond

This repository contains AI notes for my presentation on **"Explainable AI, Large Models, Neural Networks, and Beyond"**, which is a comprehensive workshop exploring the foundations, architectures, and future of modern AI systems ‚Äî from the basics of neural networks to the cutting edge of multimodal and embodied intelligence.

Author: [Davis Onyeoguzoro](https://www.linkedin.com/in/davis-onyeoguzoro/)

Date: 13th June, 2025

## Overview

This workshop aims to demystify AI by covering:

- Fundamentals of AI and its Subfields
- Neural Networks and Deep Learning Models (CNNs, RNNs, LSTM, etc.)
- Transformers and their applications in Language and Vision
- Large Language Models (LLMs) such as GPT, BERT, LLAMA, Claude
- Explainable AI (XAI) Techniques like SHAP, LIME, Attention Maps
- Bias, Fairness, and Ethical AI Development
- Multimodal AI and Embodied Intelligence
- AGI and Future Trends
- Hands-on Demos and Coding Sessions

---

## Table of Contents

- [Slides](#-slides)
- [Topics Covered](#-topics-covered)
- [Tools &amp; Demos](#Ô∏è-tools--demos)
- [Screenshots](#-screenshots)
- [Learn More](#-learn-more)

---

## Slides

Access the full slide deck used in the session:

üîó [Slides PDF](https://docs.google.com/presentation/d/1CXAfhxZO1JoozPFMJ_mp8fTWHa5QChiYAKDoLH6CuJw/edit?usp=sharing)

---

## Topics Covered

### Fundamentals

- What is AI? History and Timeline
- Subfields: ML, DL, NLP, CV, Robotics, Planning

### Neural Networks

- Feedforward, CNNs, RNNs, LSTM, GRUs
- Activation Functions: Sigmoid, ReLU, Softmax
- Forward and Backpropagation
- Loss Functions: MSE, Cross-Entropy
- Overfitting, Underfitting, Regularization

### Transformers

- Attention is All You Need (QKV & Multi-Head)
- Positional Encoding
- Transformer Architectures: Encoder, Decoder, Encoder-Decoder

### Large Language Models (LLMs)

- BERT, GPT-4, LLAMA, Claude, Gemini
- Parameter scale and what it means
- Applications: Q&A, summarization, generation

### Explainable AI (XAI)

- LIME, SHAP, Saliency & Attention Maps
- The Black Box Problem
- Trust, Debugging, Ethics, and Accountability

### Multimodal AI

- Combining Text, Audio, Images, Video
- Vision Transformers (ViT, DETR, Swin, DeiT)

### Embodied AI & AGI

- Physical AI Agents: Drones, Robots
- Planning, Perception, and Action
- Vision for the future: AGI and beyond

---

## Tools & Demos

Live Demos and Notebooks can be found here:

Demo &amp; Live Coding Repo

![Transformers Sentiment Analysis App](https://github.com/Davisonyeas/AI_Presentation/blob/main/images/Transformers_Bootcamp.png?raw=true)


Includes:

- Neural Network Forward & Backprop in code
- Visualizing Attention Maps
- LIME/SHAP interpretability examples
- Image classification with CNNs
- Sentiment analysis with Transformers

---

## Screenshots

![AI Applications](https://user-images.githubusercontent.com/example-ai-apps.png)
![Transformer Attention](https://user-images.githubusercontent.com/example-attention.png)
