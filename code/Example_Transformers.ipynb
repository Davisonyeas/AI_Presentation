{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4513177a",
   "metadata": {},
   "source": [
    "### Practical Example of Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d6e5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import math\n",
    "import torch \n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab141294",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"I think the Transformers movie is great and it reminds me of my childhood.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bef55",
   "metadata": {},
   "source": [
    "##### Brief lesson on Tokenizers\n",
    "\n",
    "It is the process of breaking down texts into smaller units (tokens). These tokens can be words, subwords, and even characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffcca1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:  {'i': 0, 'think': 1, 'the': 2, 'transformers': 3, 'movie': 4, 'is': 5, 'great': 6, 'and': 7, 'it': 8, 'reminds': 9, 'me': 10, 'of': 11, 'my': 12, 'childhood.': 13}\n",
      "inverse_vocab:  {0: 'i', 1: 'think', 2: 'the', 3: 'transformers', 4: 'movie', 5: 'is', 6: 'great', 7: 'and', 8: 'it', 9: 'reminds', 10: 'me', 11: 'of', 12: 'my', 13: 'childhood.'}\n",
      "tokens:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "# tokenizer, assign unique id to each word, without using libraries\n",
    "vocab = {w: i for i, w in enumerate(example.lower().split())}\n",
    "print(\"vocab: \", vocab)\n",
    "inverse_vocab = {i: w for w, i in vocab.items()}\n",
    "print(\"inverse_vocab: \", inverse_vocab)\n",
    "tok = torch.tensor([vocab[w] for w in example.lower().split()])\n",
    "print(\"tokens: \", tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d225a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens  ['[CLS]', 'i', 'think', 'the', 'transformers', 'movie', 'is', 'great', 'and', 'it', 'reminds', 'me', 'of', 'my', 'childhood', '.', '[SEP]']\n",
      "Libray token ids  [101, 1045, 2228, 1996, 19081, 3185, 2003, 2307, 1998, 2009, 15537, 2033, 1997, 2026, 5593, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer, using library\n",
    "lib_token = tokenizers.Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "lib_tokenized = lib_token.encode(example)\n",
    "print(\"tokens \", lib_tokenized.tokens)\n",
    "print(\"Libray token ids \", lib_tokenized.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2147d1c",
   "metadata": {},
   "source": [
    "#### Slight Deviation, but very relevant (What is [CLS] and [SEP]? )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a62811",
   "metadata": {},
   "source": [
    "[CLS] and [SEP] are special tokens that are found in BERT (reads L-R and vice verssa), they mean classification and separator. The [CLS] token is used for the context-aware representation of a word within a sentence, rather than its isolated meaning. \n",
    "\n",
    "[CLS] - Beginning and summary positiion of the sentence\n",
    "\n",
    "[SEP] - End of a single sentence, or to seperate 2 sentences in a pair\n",
    "\n",
    "Recall fromm the slides that BERT is a good example of an ENCODER-ONLY model as explained earlier, this ENCODER-ONLY layers can be fed directly into a neural-network and it would perform well in task like sentiment analysis (which is a classification tasks). \n",
    "\n",
    "Once the sentence (\"I think the transformers movie are great\") is passed to the BERT Transformer layers, the layers use Attention mechanisms to allow each token to attend to every other token in the sequence. This makes each token vector to capture richer and more complex relationships.\n",
    "\n",
    "It is at the first position in the sequence, because it captures a summary representation of the entire sentence after passing through all the transformers layers. By the end, the final vector respresentation of the CLS would have seen and know the context of the whole sentence.\n",
    "\n",
    "There are practical examples of this in the projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e5ba1",
   "metadata": {},
   "source": [
    "#### Brief lesson on Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c0d90b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library input ids:  tensor([[  101,  1045,  2228,  1996, 19081,  3185,  2003,  2307,  1998,  2009,\n",
      "         15537,  2033,  1997,  2026,  5593,  1012,   102]])\n",
      "library output last hidden state:  BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0213, -0.1722, -0.0661,  ..., -0.1890,  0.2644,  0.3204],\n",
      "         [ 0.2207, -0.0398, -0.4240,  ...,  0.0933,  0.9360,  0.4135],\n",
      "         [ 0.5625,  0.4415, -0.3280,  ..., -0.6412,  0.3912, -0.4904],\n",
      "         ...,\n",
      "         [-0.5079,  0.0307, -0.7063,  ..., -0.1720, -0.0915,  0.0143],\n",
      "         [ 0.6181, -0.0887, -0.0525,  ...,  0.2632, -0.4169, -0.3356],\n",
      "         [ 0.5937,  0.0210,  0.1422,  ...,  0.2542, -0.4829, -0.3218]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8914, -0.3729, -0.8648,  0.6573,  0.7791, -0.2078,  0.6093,  0.1718,\n",
      "         -0.8668, -0.9999, -0.5305,  0.9725,  0.9711,  0.2216,  0.8622, -0.6204,\n",
      "         -0.3324, -0.5542,  0.2032,  0.0593,  0.5623,  1.0000, -0.1269,  0.2175,\n",
      "          0.3290,  0.9902, -0.8001,  0.8943,  0.9045,  0.6677, -0.4155,  0.1986,\n",
      "         -0.9880, -0.0323, -0.8615, -0.9838,  0.4330, -0.6398,  0.1992,  0.1872,\n",
      "         -0.8911,  0.2115,  0.9999, -0.5568,  0.3487, -0.1274, -1.0000,  0.1505,\n",
      "         -0.8026,  0.9087,  0.8055,  0.9060,  0.1502,  0.3747,  0.4334, -0.3752,\n",
      "         -0.2389, -0.1019, -0.2218, -0.5265, -0.5852,  0.3735, -0.8776, -0.8259,\n",
      "          0.9269,  0.7783, -0.0913, -0.2227, -0.0480, -0.1549,  0.8210,  0.1074,\n",
      "         -0.2039, -0.8116,  0.6021,  0.2052, -0.5704,  1.0000, -0.3405, -0.9731,\n",
      "          0.9049,  0.6770,  0.4229, -0.2281,  0.2581, -1.0000,  0.6169, -0.0180,\n",
      "         -0.9877,  0.1055,  0.5915, -0.2396,  0.7113,  0.5296, -0.3030, -0.4006,\n",
      "         -0.2192, -0.8804, -0.0377, -0.3540, -0.0074, -0.1513, -0.2191, -0.2285,\n",
      "          0.1867, -0.4065, -0.2580,  0.3571,  0.3297,  0.6092,  0.3625, -0.2905,\n",
      "          0.2945, -0.9351,  0.5278, -0.2188, -0.9825, -0.5268, -0.9867,  0.5565,\n",
      "         -0.2522, -0.2010,  0.9152, -0.3083,  0.3358,  0.0385, -0.9464, -1.0000,\n",
      "         -0.6261, -0.2526, -0.2184, -0.2923, -0.9708, -0.9559,  0.5103,  0.9244,\n",
      "          0.0915,  0.9998, -0.1505,  0.9111, -0.3257, -0.5979,  0.7589, -0.3284,\n",
      "          0.5110, -0.2858, -0.4651,  0.1334, -0.3064,  0.3712, -0.7413, -0.0661,\n",
      "         -0.6654, -0.8944, -0.2887,  0.8919, -0.5847, -0.9112, -0.1242, -0.0527,\n",
      "         -0.3539,  0.7272,  0.6891,  0.1652, -0.3449,  0.3662, -0.2994,  0.3357,\n",
      "         -0.6580, -0.1051,  0.2903, -0.3508, -0.7306, -0.9748, -0.2744,  0.4683,\n",
      "          0.9785,  0.6634,  0.2063,  0.8176, -0.1336,  0.7594, -0.9416,  0.9794,\n",
      "         -0.1490,  0.2009, -0.5471,  0.6552, -0.8074, -0.0087,  0.6121, -0.8153,\n",
      "         -0.8146,  0.0780, -0.3198, -0.3217, -0.7935,  0.4316, -0.1481, -0.1830,\n",
      "          0.0229,  0.8978,  0.8587,  0.6535,  0.3529,  0.4680, -0.8672, -0.3687,\n",
      "          0.0154, -0.0789,  0.1225,  0.9918, -0.5195, -0.0683, -0.9139, -0.9795,\n",
      "         -0.1979, -0.8710, -0.0866, -0.6412,  0.5820, -0.6383,  0.3946,  0.1883,\n",
      "         -0.9221, -0.6282,  0.2804, -0.3491,  0.4311, -0.2141,  0.9750,  0.9241,\n",
      "         -0.5394,  0.3210,  0.9063, -0.9417, -0.7237,  0.4412, -0.2501,  0.7769,\n",
      "         -0.5122,  0.9800,  0.8707,  0.8259, -0.8650, -0.8122, -0.7696, -0.5321,\n",
      "         -0.0868, -0.2392,  0.8940,  0.5189,  0.4115,  0.6712, -0.4847,  0.9811,\n",
      "         -0.9913, -0.9433, -0.8143, -0.1210, -0.9839,  0.8718,  0.0440,  0.5567,\n",
      "         -0.3690, -0.5400, -0.9554,  0.6306, -0.0067,  0.9394, -0.5258, -0.7457,\n",
      "         -0.7658, -0.9206, -0.2873, -0.1218, -0.5054, -0.1260, -0.9110,  0.4348,\n",
      "          0.4501,  0.4024, -0.8171,  0.9948,  1.0000,  0.9684,  0.8073,  0.8049,\n",
      "         -0.9998, -0.9302,  1.0000, -0.9942, -1.0000, -0.8853, -0.6619,  0.1851,\n",
      "         -1.0000, -0.2134,  0.1850, -0.8937,  0.5662,  0.9722,  0.9635, -1.0000,\n",
      "          0.8346,  0.9069, -0.4616,  0.9732, -0.3655,  0.9654,  0.5876,  0.5599,\n",
      "         -0.1117,  0.3405, -0.9197, -0.6188, -0.5863, -0.8528,  0.9980,  0.0508,\n",
      "         -0.6359, -0.8370,  0.6358, -0.2059, -0.2530, -0.9447, -0.1194,  0.4792,\n",
      "          0.7797,  0.2129,  0.2316, -0.5378,  0.0386, -0.0597, -0.0518,  0.5794,\n",
      "         -0.9277, -0.3927,  0.2449,  0.0035, -0.4740, -0.9555,  0.9276, -0.3779,\n",
      "          0.7679,  1.0000,  0.5986, -0.7591,  0.5539,  0.1620, -0.2548,  1.0000,\n",
      "          0.7949, -0.9748, -0.4970,  0.6429, -0.5805, -0.5959,  0.9980, -0.2107,\n",
      "         -0.7469, -0.5507,  0.9761, -0.9874,  0.9960, -0.8151, -0.9611,  0.9552,\n",
      "          0.9121, -0.6637, -0.6834,  0.1025, -0.4084,  0.2575, -0.8977,  0.6939,\n",
      "          0.2893, -0.0022,  0.8203, -0.4191, -0.4890,  0.1931, -0.6978, -0.1473,\n",
      "          0.9180,  0.3533, -0.0128,  0.0693, -0.1135, -0.8909, -0.9642,  0.5225,\n",
      "          1.0000, -0.3440,  0.8147, -0.4966,  0.1105, -0.1054,  0.4986,  0.4853,\n",
      "         -0.1344, -0.8056,  0.8277, -0.8955, -0.9843,  0.5819,  0.1112, -0.0729,\n",
      "          1.0000,  0.3219,  0.1008,  0.6052,  0.9860, -0.2534,  0.3172,  0.8432,\n",
      "          0.9740, -0.0844,  0.5237,  0.8111, -0.8651, -0.1128, -0.5221,  0.0021,\n",
      "         -0.9123,  0.2686, -0.9394,  0.9481,  0.9063,  0.2533,  0.1138,  0.5789,\n",
      "          1.0000, -0.9014,  0.4648,  0.4238,  0.3662, -0.9998, -0.5891, -0.4095,\n",
      "          0.0334, -0.8433, -0.2751,  0.1813, -0.9562,  0.7279,  0.6931, -0.9361,\n",
      "         -0.9819, -0.5260,  0.4547,  0.0202, -0.9854, -0.5755, -0.5405,  0.6569,\n",
      "         -0.1373, -0.9283, -0.0101, -0.2151,  0.4409, -0.0933,  0.4796,  0.7618,\n",
      "          0.8516, -0.6895, -0.4167,  0.0191, -0.6899,  0.7912, -0.6736, -0.9104,\n",
      "          0.0405,  1.0000, -0.3957,  0.8607,  0.6534,  0.6973, -0.0966,  0.0271,\n",
      "          0.9113,  0.2819, -0.5949, -0.8403,  0.4283, -0.2279,  0.6195,  0.6565,\n",
      "          0.4756,  0.7262,  0.8769,  0.1708,  0.0592, -0.1094,  0.9967, -0.1253,\n",
      "         -0.3052, -0.4498,  0.1222, -0.3654,  0.3687,  1.0000,  0.1846,  0.3050,\n",
      "         -0.9856, -0.8577, -0.7837,  1.0000,  0.8377, -0.7219,  0.7505,  0.6200,\n",
      "          0.0232,  0.6080,  0.0120,  0.0010,  0.2418,  0.0016,  0.9470, -0.5683,\n",
      "         -0.9681, -0.4124,  0.3446, -0.9597,  0.9999, -0.4997, -0.2061, -0.3742,\n",
      "         -0.5525, -0.6137, -0.2625, -0.9796, -0.1398,  0.1746,  0.9455,  0.0569,\n",
      "         -0.5141, -0.8370,  0.8190,  0.8232, -0.9312, -0.9363,  0.9526, -0.9626,\n",
      "          0.7268,  1.0000,  0.2975,  0.1695,  0.0865, -0.4821,  0.3029, -0.4512,\n",
      "          0.4969, -0.9366, -0.3541, -0.1073,  0.2321, -0.0740, -0.5980,  0.4367,\n",
      "          0.1001, -0.3795, -0.6357, -0.0798,  0.4265,  0.7523, -0.1539, -0.0141,\n",
      "          0.0823, -0.0714, -0.7950, -0.3884, -0.3824, -1.0000,  0.5951, -1.0000,\n",
      "          0.5911, -0.0700, -0.1671,  0.7604,  0.8974,  0.6096, -0.6070, -0.8384,\n",
      "          0.4537,  0.6328, -0.1792, -0.2141, -0.4821,  0.2279,  0.0371,  0.2793,\n",
      "         -0.4940,  0.6182, -0.2766,  1.0000,  0.1141, -0.5335, -0.9036,  0.1607,\n",
      "         -0.1210,  1.0000, -0.7597, -0.9513,  0.2682, -0.6174, -0.6833,  0.2844,\n",
      "         -0.0046, -0.8132, -0.9539,  0.8433,  0.6364, -0.6285,  0.5615, -0.1729,\n",
      "         -0.4729, -0.1723,  0.8640,  0.9844,  0.3442,  0.7745, -0.8379, -0.3687,\n",
      "          0.9565,  0.2479, -0.1707, -0.0414,  1.0000,  0.1452, -0.8634,  0.0975,\n",
      "         -0.9628, -0.0688, -0.9385,  0.2514,  0.0819,  0.9145, -0.1952,  0.9325,\n",
      "         -0.8067, -0.0754, -0.6546, -0.3358,  0.3383, -0.8862, -0.9821, -0.9788,\n",
      "          0.6624, -0.2820,  0.1680,  0.2161, -0.0454,  0.2690,  0.4041, -1.0000,\n",
      "          0.9036,  0.1874,  0.8163,  0.9588,  0.7935,  0.6442,  0.2246, -0.9752,\n",
      "         -0.9127, -0.2756, -0.1479,  0.5279,  0.6816,  0.8044,  0.4273, -0.3838,\n",
      "         -0.7280, -0.6648, -0.9866, -0.9910,  0.2814, -0.5805, -0.8654,  0.9300,\n",
      "         -0.4417,  0.0127, -0.2191, -0.8796,  0.6818,  0.7654, -0.0546, -0.0496,\n",
      "          0.3470,  0.8265,  0.7826,  0.9586, -0.8233,  0.4672, -0.8068,  0.3705,\n",
      "          0.8990, -0.9172,  0.1471,  0.3443, -0.0974,  0.2391, -0.1071, -0.9188,\n",
      "          0.1893, -0.2385,  0.6148, -0.1585,  0.1263, -0.2531, -0.0941, -0.6314,\n",
      "         -0.6785,  0.5872,  0.1238,  0.8558,  0.9042,  0.0223, -0.5307, -0.1232,\n",
      "         -0.6671, -0.8985,  0.7424,  0.0696,  0.0578,  0.4754, -0.1367,  0.9022,\n",
      "          0.1188, -0.3461, -0.3645, -0.6909,  0.7771, -0.7257, -0.4599, -0.5500,\n",
      "          0.7632,  0.2003,  1.0000, -0.7097, -0.9120, -0.5287, -0.3396,  0.2463,\n",
      "         -0.3019, -1.0000,  0.2254, -0.7657,  0.6414, -0.8076,  0.8691, -0.6876,\n",
      "         -0.9469, -0.0522,  0.7125,  0.7811, -0.4254, -0.6701,  0.4501, -0.3492,\n",
      "          0.9867,  0.7694,  0.1706,  0.2782,  0.6086, -0.7868, -0.6263,  0.8690]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=(tensor([[[[4.1490e-02, 3.5157e-02, 1.9550e-02,  ..., 4.3124e-02,\n",
      "           8.2842e-02, 2.1068e-01],\n",
      "          [8.7553e-02, 6.1498e-02, 2.9578e-02,  ..., 5.7820e-02,\n",
      "           3.8716e-02, 4.4489e-02],\n",
      "          [5.5552e-02, 1.3856e-01, 1.2060e-02,  ..., 4.5218e-02,\n",
      "           5.9846e-02, 6.1225e-02],\n",
      "          ...,\n",
      "          [3.1202e-02, 9.4379e-02, 6.0256e-02,  ..., 8.0667e-02,\n",
      "           3.8146e-02, 3.4898e-02],\n",
      "          [4.7171e-02, 4.4481e-02, 4.2139e-02,  ..., 6.1077e-02,\n",
      "           1.0464e-01, 5.4915e-02],\n",
      "          [4.6869e-02, 5.5916e-02, 2.4929e-02,  ..., 4.4511e-02,\n",
      "           1.0539e-01, 7.5599e-02]],\n",
      "\n",
      "         [[4.2623e-01, 9.9760e-03, 1.9682e-03,  ..., 1.3849e-03,\n",
      "           2.1772e-02, 4.4303e-03],\n",
      "          [2.5702e-03, 6.6599e-03, 1.6099e-01,  ..., 2.0518e-01,\n",
      "           8.0194e-03, 1.0785e-02],\n",
      "          [1.1493e-02, 9.1171e-03, 8.3256e-02,  ..., 1.1055e-01,\n",
      "           1.6726e-02, 4.9148e-02],\n",
      "          ...,\n",
      "          [2.2423e-02, 1.2494e-02, 5.0382e-02,  ..., 2.2712e-01,\n",
      "           1.9198e-02, 4.0492e-02],\n",
      "          [5.7514e-04, 8.4424e-02, 1.6412e-02,  ..., 1.2186e-02,\n",
      "           1.7208e-01, 5.2463e-03],\n",
      "          [1.4872e-02, 6.7015e-02, 1.2931e-02,  ..., 5.5462e-03,\n",
      "           1.8795e-01, 9.9254e-03]],\n",
      "\n",
      "         [[6.8461e-01, 2.7510e-02, 1.4389e-02,  ..., 1.3542e-02,\n",
      "           2.6454e-02, 4.3446e-02],\n",
      "          [7.5224e-01, 7.1339e-02, 1.2694e-02,  ..., 8.1727e-03,\n",
      "           1.0985e-02, 2.4861e-02],\n",
      "          [9.3558e-02, 8.2654e-01, 1.7666e-03,  ..., 2.0102e-03,\n",
      "           1.2943e-02, 6.7860e-03],\n",
      "          ...,\n",
      "          [2.8184e-01, 1.2841e-03, 4.6591e-03,  ..., 1.3906e-02,\n",
      "           3.7849e-02, 4.6314e-02],\n",
      "          [1.2080e-01, 1.6606e-02, 7.5935e-04,  ..., 4.0947e-01,\n",
      "           1.8632e-01, 8.5472e-02],\n",
      "          [2.7103e-01, 1.2836e-02, 1.1887e-03,  ..., 3.2498e-02,\n",
      "           4.9900e-01, 1.2547e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.8424e-02, 3.7948e-02, 1.1655e-01,  ..., 4.6805e-02,\n",
      "           2.6224e-02, 1.1563e-02],\n",
      "          [1.0114e-01, 1.7515e-01, 1.4471e-02,  ..., 3.3887e-02,\n",
      "           1.4691e-02, 5.3316e-02],\n",
      "          [1.1134e-01, 3.1721e-02, 1.4421e-01,  ..., 1.2717e-01,\n",
      "           3.2536e-02, 5.1884e-02],\n",
      "          ...,\n",
      "          [2.0226e-01, 1.2031e-02, 3.8678e-02,  ..., 2.7282e-01,\n",
      "           9.3159e-03, 4.9963e-02],\n",
      "          [4.7896e-01, 9.7336e-03, 3.1701e-02,  ..., 2.6130e-02,\n",
      "           1.7118e-02, 8.3427e-02],\n",
      "          [4.3684e-01, 2.7460e-02, 2.8623e-02,  ..., 3.5235e-02,\n",
      "           3.7420e-02, 2.6837e-02]],\n",
      "\n",
      "         [[7.1143e-01, 3.5467e-02, 1.6603e-02,  ..., 8.8276e-03,\n",
      "           1.8108e-02, 1.9862e-02],\n",
      "          [3.8583e-04, 3.0900e-03, 9.4771e-01,  ..., 4.4806e-04,\n",
      "           3.0360e-04, 2.1200e-03],\n",
      "          [4.3592e-02, 6.6460e-02, 1.5215e-01,  ..., 3.1795e-03,\n",
      "           3.3012e-03, 5.3574e-03],\n",
      "          ...,\n",
      "          [9.1170e-02, 7.8867e-03, 1.0252e-03,  ..., 3.7496e-02,\n",
      "           5.1703e-01, 2.1443e-01],\n",
      "          [4.3327e-03, 7.4287e-04, 1.8337e-03,  ..., 1.7484e-02,\n",
      "           3.8178e-02, 9.2124e-01],\n",
      "          [1.1443e-01, 3.5798e-03, 2.0554e-03,  ..., 7.1352e-02,\n",
      "           3.6625e-01, 3.7615e-01]],\n",
      "\n",
      "         [[7.2470e-01, 1.3091e-02, 4.3810e-03,  ..., 2.8101e-03,\n",
      "           2.6122e-07, 2.7051e-02],\n",
      "          [2.5473e-01, 4.2915e-02, 6.5324e-02,  ..., 2.1124e-02,\n",
      "           1.7138e-02, 9.4062e-02],\n",
      "          [2.0303e-01, 2.1174e-01, 3.4722e-02,  ..., 6.9947e-02,\n",
      "           2.1497e-02, 2.5829e-02],\n",
      "          ...,\n",
      "          [2.4099e-01, 6.7666e-03, 5.1970e-02,  ..., 7.6926e-02,\n",
      "           1.9741e-02, 2.0166e-01],\n",
      "          [3.9439e-01, 1.2071e-02, 3.5718e-02,  ..., 7.7554e-02,\n",
      "           1.5713e-03, 2.1814e-01],\n",
      "          [5.4771e-01, 3.8316e-02, 1.0108e-02,  ..., 2.4876e-02,\n",
      "           1.5057e-03, 1.8612e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.5808e-01, 7.6405e-03, 6.3266e-03,  ..., 4.0075e-02,\n",
      "           2.4668e-01, 3.7587e-02],\n",
      "          [3.2126e-02, 3.3957e-02, 1.9609e-01,  ..., 2.7845e-03,\n",
      "           4.0774e-03, 2.2005e-02],\n",
      "          [6.2344e-02, 3.2132e-02, 1.4347e-01,  ..., 9.9602e-03,\n",
      "           2.2208e-02, 2.6780e-02],\n",
      "          ...,\n",
      "          [4.9629e-01, 3.0701e-02, 3.6332e-03,  ..., 3.6802e-03,\n",
      "           5.4685e-02, 1.8440e-01],\n",
      "          [5.5665e-01, 1.6416e-02, 9.1327e-03,  ..., 2.6074e-02,\n",
      "           1.0501e-01, 4.2171e-02],\n",
      "          [4.9813e-01, 5.0506e-03, 1.1851e-02,  ..., 2.7688e-02,\n",
      "           1.2395e-01, 4.6707e-02]],\n",
      "\n",
      "         [[3.8313e-01, 5.7993e-02, 2.5188e-02,  ..., 1.8219e-02,\n",
      "           5.0242e-02, 6.4765e-02],\n",
      "          [1.1209e-01, 5.2689e-03, 8.4706e-01,  ..., 2.0807e-05,\n",
      "           1.5746e-03, 1.9973e-02],\n",
      "          [4.6361e-01, 1.4667e-02, 7.4897e-02,  ..., 1.1770e-03,\n",
      "           2.9516e-03, 2.3726e-02],\n",
      "          ...,\n",
      "          [9.0100e-01, 1.6685e-03, 1.9207e-05,  ..., 3.2638e-03,\n",
      "           4.7118e-02, 4.2960e-02],\n",
      "          [6.6181e-01, 4.6288e-05, 9.4723e-05,  ..., 4.7061e-04,\n",
      "           1.0133e-02, 3.2632e-01],\n",
      "          [9.0811e-01, 5.7811e-04, 2.6346e-04,  ..., 1.8411e-03,\n",
      "           4.0393e-03, 8.1903e-02]],\n",
      "\n",
      "         [[8.2970e-01, 7.9720e-03, 6.8777e-03,  ..., 2.1896e-03,\n",
      "           1.9787e-02, 2.5930e-02],\n",
      "          [2.4288e-01, 6.7779e-02, 1.6972e-02,  ..., 5.8573e-02,\n",
      "           1.9748e-02, 1.4798e-01],\n",
      "          [3.7568e-01, 6.8091e-02, 1.1090e-02,  ..., 1.2686e-02,\n",
      "           1.5977e-02, 1.6941e-01],\n",
      "          ...,\n",
      "          [9.0051e-01, 3.9833e-03, 7.9750e-03,  ..., 1.1864e-03,\n",
      "           1.8859e-02, 1.0339e-02],\n",
      "          [4.5992e-01, 1.0704e-01, 4.2511e-02,  ..., 1.1518e-02,\n",
      "           2.4473e-02, 1.3626e-01],\n",
      "          [8.4422e-01, 1.7172e-02, 9.3939e-03,  ..., 1.1097e-03,\n",
      "           1.1894e-02, 3.7634e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.6521e-01, 2.5934e-02, 2.8200e-02,  ..., 2.6548e-02,\n",
      "           9.4517e-02, 1.0413e-01],\n",
      "          [5.5937e-01, 1.3650e-02, 3.9930e-03,  ..., 5.5608e-03,\n",
      "           4.1422e-02, 1.4607e-01],\n",
      "          [6.6423e-01, 6.2134e-03, 1.1052e-03,  ..., 1.0342e-02,\n",
      "           3.6389e-02, 1.1294e-01],\n",
      "          ...,\n",
      "          [5.6085e-01, 1.4532e-02, 5.3771e-03,  ..., 5.9843e-04,\n",
      "           3.8853e-02, 1.7015e-01],\n",
      "          [2.7296e-01, 1.9967e-02, 3.2067e-02,  ..., 4.7073e-02,\n",
      "           7.2634e-02, 1.2763e-01],\n",
      "          [1.2173e-01, 2.3465e-02, 2.2582e-02,  ..., 4.1875e-02,\n",
      "           8.5875e-02, 1.4074e-01]],\n",
      "\n",
      "         [[5.6984e-01, 2.6967e-02, 1.9267e-02,  ..., 1.4812e-02,\n",
      "           4.4930e-02, 7.2143e-02],\n",
      "          [8.0098e-01, 3.4156e-02, 1.2580e-02,  ..., 1.5934e-03,\n",
      "           1.5875e-03, 2.7998e-03],\n",
      "          [5.2529e-01, 2.0112e-01, 7.7121e-03,  ..., 1.0625e-02,\n",
      "           5.2351e-03, 4.8668e-03],\n",
      "          ...,\n",
      "          [1.0822e-01, 2.1614e-03, 8.3252e-03,  ..., 4.0342e-02,\n",
      "           5.8684e-02, 7.4066e-02],\n",
      "          [1.1999e-01, 2.5042e-03, 1.4382e-02,  ..., 2.5649e-02,\n",
      "           7.9756e-02, 5.9938e-02],\n",
      "          [8.9809e-01, 2.3746e-04, 1.0575e-04,  ..., 6.1478e-03,\n",
      "           2.4137e-02, 3.1673e-02]],\n",
      "\n",
      "         [[2.4916e-01, 3.1221e-02, 2.9553e-02,  ..., 2.6926e-02,\n",
      "           4.5149e-02, 1.6757e-01],\n",
      "          [1.0022e-01, 7.4990e-02, 5.6655e-03,  ..., 2.0079e-03,\n",
      "           4.4444e-02, 1.0407e-01],\n",
      "          [1.2577e-01, 3.5321e-02, 2.2305e-01,  ..., 1.4292e-02,\n",
      "           6.3551e-02, 8.2086e-02],\n",
      "          ...,\n",
      "          [9.6358e-02, 1.1984e-03, 4.6828e-03,  ..., 5.2082e-01,\n",
      "           3.1606e-02, 6.9624e-02],\n",
      "          [1.3926e-01, 4.2161e-02, 4.9274e-02,  ..., 1.1018e-02,\n",
      "           1.0469e-02, 4.9265e-02],\n",
      "          [2.2999e-01, 3.7030e-02, 3.1624e-02,  ..., 2.5312e-02,\n",
      "           3.6328e-02, 5.4370e-02]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[9.8979e-01, 1.6086e-04, 8.3238e-05,  ..., 6.3807e-06,\n",
      "           1.1728e-03, 6.8261e-03],\n",
      "          [5.2929e-07, 2.5975e-06, 9.9999e-01,  ..., 6.0574e-12,\n",
      "           3.0435e-10, 4.4330e-07],\n",
      "          [6.1894e-07, 6.6146e-07, 6.3751e-06,  ..., 1.5897e-08,\n",
      "           3.3511e-12, 1.3420e-08],\n",
      "          ...,\n",
      "          [2.0491e-05, 3.7007e-07, 5.2430e-11,  ..., 1.4413e-06,\n",
      "           9.9989e-01, 2.0729e-05],\n",
      "          [6.8690e-04, 1.1163e-06, 7.6734e-06,  ..., 1.4335e-06,\n",
      "           1.5597e-03, 9.9756e-01],\n",
      "          [9.7654e-01, 2.9869e-06, 3.3502e-05,  ..., 5.1108e-06,\n",
      "           6.0011e-04, 2.2341e-02]],\n",
      "\n",
      "         [[7.1936e-01, 3.6889e-03, 2.8092e-03,  ..., 2.8131e-03,\n",
      "           2.3247e-02, 1.3609e-01],\n",
      "          [8.2613e-01, 1.5203e-02, 2.0041e-02,  ..., 8.5010e-04,\n",
      "           6.7581e-03, 6.3503e-02],\n",
      "          [6.3629e-01, 2.0274e-01, 8.0291e-04,  ..., 4.5294e-04,\n",
      "           1.0292e-02, 1.0307e-01],\n",
      "          ...,\n",
      "          [2.8832e-01, 1.7028e-04, 6.2139e-05,  ..., 3.0288e-03,\n",
      "           7.1882e-02, 1.9258e-01],\n",
      "          [6.1588e-01, 3.5915e-03, 2.1619e-03,  ..., 1.9623e-02,\n",
      "           6.0721e-02, 1.4535e-01],\n",
      "          [7.7078e-01, 1.4504e-03, 5.1251e-04,  ..., 6.6950e-03,\n",
      "           2.7072e-02, 1.6363e-01]],\n",
      "\n",
      "         [[7.1998e-01, 6.7790e-03, 4.2902e-03,  ..., 7.4469e-03,\n",
      "           3.1897e-02, 7.5646e-02],\n",
      "          [5.9333e-01, 1.5883e-02, 6.7366e-03,  ..., 1.0655e-02,\n",
      "           5.1204e-02, 1.2948e-01],\n",
      "          [8.9682e-01, 3.8989e-04, 1.3550e-03,  ..., 1.1644e-03,\n",
      "           2.8820e-02, 3.3219e-02],\n",
      "          ...,\n",
      "          [3.7713e-01, 4.8146e-03, 1.9645e-03,  ..., 6.4378e-03,\n",
      "           9.3073e-02, 2.8810e-01],\n",
      "          [4.5979e-01, 1.7398e-02, 1.5947e-02,  ..., 2.9261e-02,\n",
      "           4.6465e-02, 7.4886e-02],\n",
      "          [3.7461e-01, 1.9779e-02, 1.2114e-02,  ..., 1.9494e-02,\n",
      "           4.9268e-02, 1.0077e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[8.5745e-01, 5.8089e-03, 2.6666e-03,  ..., 5.6353e-04,\n",
      "           2.0726e-02, 8.0348e-02],\n",
      "          [4.3270e-07, 3.5176e-06, 9.9999e-01,  ..., 3.4753e-12,\n",
      "           2.8513e-10, 1.6898e-07],\n",
      "          [6.2982e-07, 9.4654e-07, 2.6753e-06,  ..., 1.6932e-08,\n",
      "           4.9556e-12, 4.8070e-08],\n",
      "          ...,\n",
      "          [1.9720e-05, 1.0522e-06, 7.6752e-11,  ..., 4.0652e-07,\n",
      "           9.9991e-01, 2.2072e-05],\n",
      "          [8.4055e-04, 1.3001e-06, 2.1189e-06,  ..., 5.4008e-07,\n",
      "           1.1613e-03, 9.9789e-01],\n",
      "          [9.9060e-01, 3.8863e-07, 2.7117e-05,  ..., 3.8279e-05,\n",
      "           1.7185e-03, 7.4093e-03]],\n",
      "\n",
      "         [[5.5319e-01, 1.9472e-02, 1.1673e-02,  ..., 1.5172e-02,\n",
      "           3.5640e-02, 1.5651e-01],\n",
      "          [2.4386e-01, 4.3571e-02, 2.0318e-02,  ..., 1.1971e-02,\n",
      "           4.3753e-02, 4.8234e-02],\n",
      "          [6.4419e-01, 3.8632e-02, 1.2488e-02,  ..., 1.1053e-02,\n",
      "           2.6755e-02, 1.6715e-02],\n",
      "          ...,\n",
      "          [2.0535e-01, 8.4903e-03, 4.0347e-02,  ..., 7.0462e-02,\n",
      "           6.6836e-02, 3.7268e-02],\n",
      "          [6.8127e-01, 3.2563e-02, 1.4216e-02,  ..., 1.0180e-02,\n",
      "           1.9740e-02, 6.2667e-02],\n",
      "          [8.1582e-01, 6.1398e-03, 5.8665e-03,  ..., 6.9897e-03,\n",
      "           2.3560e-02, 5.8149e-02]],\n",
      "\n",
      "         [[9.4887e-01, 7.8261e-04, 8.1825e-04,  ..., 1.4629e-03,\n",
      "           6.6558e-03, 1.8862e-02],\n",
      "          [1.0216e-01, 2.1359e-02, 1.7342e-01,  ..., 4.9123e-02,\n",
      "           3.9914e-02, 2.4353e-02],\n",
      "          [1.1465e-01, 1.2132e-02, 1.8896e-02,  ..., 1.6508e-02,\n",
      "           2.8093e-02, 1.8283e-02],\n",
      "          ...,\n",
      "          [4.5905e-01, 2.4928e-02, 2.4333e-02,  ..., 4.1915e-02,\n",
      "           7.3312e-02, 8.4776e-02],\n",
      "          [5.7551e-01, 6.2532e-03, 3.1378e-03,  ..., 1.1164e-02,\n",
      "           7.3610e-02, 1.9861e-01],\n",
      "          [9.8132e-01, 1.8493e-04, 2.4155e-04,  ..., 3.8810e-04,\n",
      "           2.8295e-03, 6.0282e-03]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.6802e-01, 1.3503e-03, 2.5241e-03,  ..., 2.4539e-03,\n",
      "           4.2294e-02, 5.4491e-01],\n",
      "          [1.7557e-01, 1.0482e-01, 4.0299e-03,  ..., 4.4778e-03,\n",
      "           6.9758e-02, 2.9308e-01],\n",
      "          [5.7154e-01, 6.1500e-04, 4.6166e-02,  ..., 2.7884e-05,\n",
      "           5.2847e-02, 3.1911e-01],\n",
      "          ...,\n",
      "          [5.1716e-01, 6.6997e-04, 7.7428e-05,  ..., 5.0378e-02,\n",
      "           3.4031e-02, 3.8999e-01],\n",
      "          [4.2501e-01, 2.4910e-03, 6.0739e-03,  ..., 7.5311e-04,\n",
      "           6.0634e-02, 4.5211e-01],\n",
      "          [3.1818e-01, 1.2508e-03, 4.2508e-03,  ..., 8.0930e-03,\n",
      "           1.9706e-02, 5.9390e-01]],\n",
      "\n",
      "         [[4.2439e-01, 8.4732e-03, 5.4744e-03,  ..., 4.2127e-03,\n",
      "           4.8020e-02, 4.5932e-01],\n",
      "          [1.0303e-01, 8.0629e-02, 3.8850e-02,  ..., 4.1559e-02,\n",
      "           8.5601e-02, 2.3885e-01],\n",
      "          [1.3396e-01, 6.7861e-02, 1.3056e-02,  ..., 7.4310e-02,\n",
      "           2.9102e-02, 1.8732e-01],\n",
      "          ...,\n",
      "          [1.7717e-01, 2.3463e-02, 1.7685e-02,  ..., 5.4848e-03,\n",
      "           2.8999e-02, 1.6396e-01],\n",
      "          [6.6517e-02, 1.2830e-02, 6.7093e-03,  ..., 1.7874e-02,\n",
      "           5.8913e-02, 7.4482e-01],\n",
      "          [4.8988e-01, 1.8029e-03, 3.5999e-03,  ..., 5.5180e-04,\n",
      "           3.1472e-02, 4.4986e-01]],\n",
      "\n",
      "         [[6.7919e-02, 5.1278e-02, 3.9776e-02,  ..., 5.6856e-02,\n",
      "           5.8810e-02, 1.3135e-01],\n",
      "          [9.5901e-02, 1.1616e-01, 2.8899e-03,  ..., 2.3014e-03,\n",
      "           2.8411e-01, 1.4573e-01],\n",
      "          [2.6169e-01, 7.9682e-03, 1.2485e-03,  ..., 1.0460e-03,\n",
      "           2.3506e-01, 2.8553e-01],\n",
      "          ...,\n",
      "          [2.2066e-01, 8.6095e-03, 1.3996e-02,  ..., 1.4633e-02,\n",
      "           7.1238e-02, 2.6662e-01],\n",
      "          [2.5915e-01, 5.3699e-03, 3.7529e-03,  ..., 5.4790e-03,\n",
      "           3.0584e-01, 2.1161e-01],\n",
      "          [1.3146e-01, 3.5321e-03, 4.4647e-03,  ..., 5.3050e-03,\n",
      "           1.0543e-02, 7.1997e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.8475e-01, 4.2097e-03, 5.9926e-03,  ..., 2.0899e-03,\n",
      "           2.6680e-02, 1.2133e-01],\n",
      "          [3.2228e-02, 2.7122e-03, 8.8360e-01,  ..., 6.0410e-05,\n",
      "           5.5195e-05, 6.6963e-02],\n",
      "          [1.0263e-01, 4.1797e-03, 2.4235e-02,  ..., 4.5909e-03,\n",
      "           6.2023e-04, 2.3704e-01],\n",
      "          ...,\n",
      "          [7.2481e-01, 2.4781e-04, 1.7821e-04,  ..., 1.2105e-03,\n",
      "           1.4903e-01, 1.1317e-01],\n",
      "          [7.9890e-01, 4.5510e-04, 9.3880e-04,  ..., 1.5631e-03,\n",
      "           3.0159e-02, 1.6273e-01],\n",
      "          [8.6519e-01, 4.2351e-03, 5.2416e-03,  ..., 2.0561e-03,\n",
      "           2.4902e-02, 5.7549e-02]],\n",
      "\n",
      "         [[3.6321e-01, 2.0585e-03, 2.7360e-03,  ..., 2.9414e-03,\n",
      "           5.5920e-03, 5.8628e-01],\n",
      "          [1.5993e-01, 4.2950e-02, 1.5788e-01,  ..., 3.7306e-02,\n",
      "           1.7853e-02, 1.4361e-01],\n",
      "          [1.9975e-01, 1.9503e-02, 1.8778e-02,  ..., 1.5548e-02,\n",
      "           9.8142e-03, 2.3778e-01],\n",
      "          ...,\n",
      "          [3.4017e-01, 5.0452e-03, 4.2147e-03,  ..., 8.0371e-02,\n",
      "           2.4500e-02, 3.6173e-01],\n",
      "          [1.7417e-01, 3.7321e-02, 3.7418e-02,  ..., 5.6181e-02,\n",
      "           3.4572e-02, 2.3714e-01],\n",
      "          [3.3688e-01, 7.3439e-04, 1.8277e-03,  ..., 6.1260e-04,\n",
      "           4.0191e-03, 6.4252e-01]],\n",
      "\n",
      "         [[7.1408e-01, 3.7049e-03, 3.3644e-03,  ..., 5.5741e-03,\n",
      "           4.4110e-02, 1.7770e-01],\n",
      "          [8.3465e-01, 1.5909e-02, 5.5416e-03,  ..., 8.3769e-04,\n",
      "           4.0966e-03, 9.5713e-02],\n",
      "          [7.2358e-01, 6.7376e-02, 1.0475e-02,  ..., 8.1377e-04,\n",
      "           4.1970e-03, 1.3663e-01],\n",
      "          ...,\n",
      "          [7.3147e-02, 1.1981e-03, 3.5173e-03,  ..., 1.3793e-02,\n",
      "           2.6899e-02, 2.2714e-01],\n",
      "          [8.7364e-02, 1.9035e-03, 5.1877e-03,  ..., 9.0980e-02,\n",
      "           1.4405e-01, 2.5359e-01],\n",
      "          [8.2581e-01, 8.5170e-04, 7.0227e-04,  ..., 1.0572e-03,\n",
      "           1.3283e-02, 1.4279e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[6.5880e-02, 2.1604e-03, 3.0494e-03,  ..., 2.1158e-03,\n",
      "           2.2033e-02, 8.6332e-01],\n",
      "          [2.0102e-02, 1.0604e-02, 9.7813e-02,  ..., 4.0012e-02,\n",
      "           2.3677e-02, 6.0519e-01],\n",
      "          [2.0566e-02, 1.4847e-02, 2.9108e-02,  ..., 1.1476e-02,\n",
      "           3.0900e-02, 5.4489e-01],\n",
      "          ...,\n",
      "          [1.0298e-02, 2.6572e-03, 7.9793e-03,  ..., 3.4444e-02,\n",
      "           5.3677e-03, 8.0753e-01],\n",
      "          [2.2630e-02, 8.1581e-03, 1.4288e-02,  ..., 3.1383e-03,\n",
      "           8.9872e-02, 5.6979e-01],\n",
      "          [5.2356e-02, 9.2989e-04, 2.7363e-03,  ..., 7.1516e-04,\n",
      "           2.0647e-01, 7.1913e-01]],\n",
      "\n",
      "         [[1.2305e-02, 7.1629e-04, 3.9747e-03,  ..., 1.6318e-02,\n",
      "           6.0356e-03, 1.4277e-03],\n",
      "          [1.1550e-03, 3.4347e-03, 8.0545e-03,  ..., 3.5904e-02,\n",
      "           9.2072e-03, 1.8090e-02],\n",
      "          [1.4705e-02, 8.9854e-03, 1.5372e-02,  ..., 2.5673e-02,\n",
      "           6.7301e-02, 1.1779e-01],\n",
      "          ...,\n",
      "          [4.0233e-03, 2.5464e-02, 1.9001e-02,  ..., 2.9260e-02,\n",
      "           1.1078e-02, 4.1973e-01],\n",
      "          [3.7294e-03, 2.9332e-02, 1.4637e-02,  ..., 1.0993e-01,\n",
      "           1.1573e-01, 1.6259e-02],\n",
      "          [5.3713e-02, 1.9247e-02, 2.8566e-02,  ..., 4.2989e-02,\n",
      "           1.7496e-02, 5.8893e-01]],\n",
      "\n",
      "         [[4.5533e-02, 6.4037e-02, 3.1365e-02,  ..., 8.4736e-02,\n",
      "           3.6301e-02, 7.0458e-02],\n",
      "          [3.7707e-02, 1.2424e-02, 8.2439e-03,  ..., 3.2492e-02,\n",
      "           2.2669e-02, 2.1186e-01],\n",
      "          [6.0178e-02, 3.2291e-02, 8.9705e-03,  ..., 2.2332e-02,\n",
      "           4.8707e-02, 6.6205e-01],\n",
      "          ...,\n",
      "          [6.7418e-02, 1.0869e-02, 6.9593e-03,  ..., 1.5339e-03,\n",
      "           3.3390e-02, 7.5996e-01],\n",
      "          [1.1883e-01, 9.0025e-02, 5.4852e-02,  ..., 1.3281e-02,\n",
      "           4.0185e-02, 8.7900e-02],\n",
      "          [9.1582e-02, 8.1161e-03, 7.4807e-03,  ..., 2.7060e-03,\n",
      "           2.2117e-02, 7.8909e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.6138e-02, 1.1299e-02, 1.4591e-02,  ..., 3.8429e-02,\n",
      "           1.2356e-02, 5.8272e-01],\n",
      "          [1.5523e-01, 1.4402e-02, 1.1169e-02,  ..., 1.6767e-02,\n",
      "           2.9843e-02, 4.3007e-01],\n",
      "          [7.7738e-02, 2.0499e-02, 2.3204e-02,  ..., 2.4279e-02,\n",
      "           4.9660e-02, 4.8393e-01],\n",
      "          ...,\n",
      "          [1.2505e-01, 1.8003e-03, 9.7606e-03,  ..., 3.4053e-03,\n",
      "           4.2749e-02, 4.6718e-01],\n",
      "          [9.8561e-02, 2.6029e-02, 2.9710e-02,  ..., 5.7270e-03,\n",
      "           1.2480e-01, 3.4520e-01],\n",
      "          [2.9577e-02, 1.7739e-03, 3.3401e-03,  ..., 5.9082e-03,\n",
      "           9.3365e-03, 8.8891e-01]],\n",
      "\n",
      "         [[1.3139e-02, 5.0556e-03, 2.8447e-03,  ..., 4.3548e-03,\n",
      "           2.1407e-02, 9.1625e-01],\n",
      "          [6.3249e-02, 3.2587e-02, 1.8680e-02,  ..., 4.6194e-03,\n",
      "           3.0859e-02, 8.2127e-01],\n",
      "          [2.4552e-02, 3.2647e-01, 5.1554e-02,  ..., 2.5399e-03,\n",
      "           2.7718e-02, 5.3684e-01],\n",
      "          ...,\n",
      "          [4.6071e-02, 4.0253e-03, 5.7780e-03,  ..., 2.7889e-02,\n",
      "           1.8492e-02, 4.4808e-01],\n",
      "          [5.1328e-02, 2.6326e-02, 2.2216e-02,  ..., 2.0013e-01,\n",
      "           1.9029e-02, 5.6953e-02],\n",
      "          [1.0204e-02, 1.3645e-03, 1.1937e-03,  ..., 3.8974e-03,\n",
      "           6.5922e-03, 9.5736e-01]],\n",
      "\n",
      "         [[4.5616e-02, 3.2566e-01, 3.6785e-02,  ..., 7.8031e-03,\n",
      "           4.2511e-02, 3.7808e-01],\n",
      "          [3.0067e-02, 3.2263e-02, 5.3911e-01,  ..., 1.5129e-03,\n",
      "           8.7936e-04, 3.7517e-01],\n",
      "          [5.0403e-02, 1.4102e-02, 4.0605e-02,  ..., 7.1444e-04,\n",
      "           1.6173e-03, 7.6994e-01],\n",
      "          ...,\n",
      "          [2.7038e-02, 3.1569e-03, 1.2940e-03,  ..., 5.6193e-03,\n",
      "           2.6335e-01, 6.7246e-01],\n",
      "          [1.2412e-02, 4.2336e-01, 2.4609e-02,  ..., 2.7830e-03,\n",
      "           1.2138e-01, 3.6570e-01],\n",
      "          [1.3735e-02, 1.6579e-02, 3.9911e-03,  ..., 4.8291e-03,\n",
      "           1.5638e-02, 8.8641e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.6545e-02, 2.9653e-02, 6.2482e-02,  ..., 3.2785e-03,\n",
      "           1.2156e-02, 7.2652e-01],\n",
      "          [3.4367e-02, 2.7518e-02, 3.4811e-01,  ..., 6.7421e-04,\n",
      "           3.1300e-03, 4.8387e-01],\n",
      "          [4.0076e-02, 2.5409e-02, 4.8594e-02,  ..., 8.8874e-04,\n",
      "           4.6034e-03, 5.5722e-01],\n",
      "          ...,\n",
      "          [1.0002e-02, 2.7366e-03, 2.4305e-03,  ..., 3.5374e-02,\n",
      "           4.5996e-02, 8.3788e-01],\n",
      "          [5.9490e-02, 4.4249e-02, 3.4444e-02,  ..., 4.4692e-03,\n",
      "           4.7385e-02, 6.4406e-01],\n",
      "          [1.8573e-02, 1.9455e-03, 2.9777e-03,  ..., 3.9416e-03,\n",
      "           1.1536e-02, 9.2143e-01]],\n",
      "\n",
      "         [[1.6738e-02, 1.0043e-02, 1.7743e-03,  ..., 3.4340e-03,\n",
      "           1.3143e-02, 9.0877e-01],\n",
      "          [2.2765e-03, 3.1178e-03, 5.5507e-04,  ..., 5.6657e-04,\n",
      "           2.2454e-03, 9.7974e-01],\n",
      "          [8.0332e-03, 1.4268e-01, 8.3080e-04,  ..., 5.1284e-04,\n",
      "           4.6250e-03, 8.3325e-01],\n",
      "          ...,\n",
      "          [2.4409e-03, 5.3122e-04, 1.1748e-05,  ..., 5.6234e-03,\n",
      "           2.8233e-03, 9.7406e-01],\n",
      "          [2.2003e-02, 2.6432e-03, 1.5520e-03,  ..., 1.8447e-02,\n",
      "           2.3556e-02, 8.9821e-01],\n",
      "          [6.2114e-03, 1.0788e-03, 3.3538e-04,  ..., 3.2736e-04,\n",
      "           2.8126e-03, 9.8124e-01]],\n",
      "\n",
      "         [[3.1543e-02, 4.1674e-03, 3.7705e-03,  ..., 6.1896e-03,\n",
      "           2.7218e-02, 8.2099e-01],\n",
      "          [7.1325e-02, 3.2514e-02, 1.9138e-02,  ..., 1.5245e-02,\n",
      "           1.8151e-01, 3.8300e-01],\n",
      "          [5.7062e-02, 1.2199e-02, 1.1927e-02,  ..., 3.5017e-02,\n",
      "           5.8640e-02, 5.5307e-01],\n",
      "          ...,\n",
      "          [3.5139e-02, 1.8424e-02, 3.2702e-02,  ..., 2.5867e-02,\n",
      "           3.8017e-02, 4.2207e-01],\n",
      "          [2.2107e-01, 2.4725e-02, 1.7283e-02,  ..., 7.0395e-03,\n",
      "           1.3154e-01, 2.5207e-01],\n",
      "          [2.1280e-02, 1.8750e-03, 1.3856e-03,  ..., 8.9565e-04,\n",
      "           5.1002e-03, 9.4504e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.5596e-02, 8.1204e-02, 7.0846e-02,  ..., 3.4775e-03,\n",
      "           7.3021e-02, 6.4949e-01],\n",
      "          [3.9260e-04, 4.8851e-03, 7.9189e-01,  ..., 1.1385e-04,\n",
      "           2.0288e-04, 1.9747e-01],\n",
      "          [1.9695e-03, 3.3193e-03, 5.7659e-03,  ..., 4.8021e-04,\n",
      "           5.7724e-04, 8.4528e-01],\n",
      "          ...,\n",
      "          [3.2298e-02, 4.3266e-03, 3.4352e-03,  ..., 1.0931e-02,\n",
      "           1.9185e-01, 7.4051e-01],\n",
      "          [1.0858e-02, 2.5999e-02, 1.6989e-02,  ..., 2.9930e-03,\n",
      "           3.9339e-02, 8.4475e-01],\n",
      "          [1.9482e-01, 4.6295e-03, 2.8903e-03,  ..., 2.0084e-02,\n",
      "           6.4002e-02, 6.5435e-01]],\n",
      "\n",
      "         [[1.4882e-02, 5.7919e-03, 2.0487e-03,  ..., 2.2939e-02,\n",
      "           7.4286e-02, 7.2430e-01],\n",
      "          [2.5067e-01, 3.7012e-02, 1.7623e-02,  ..., 1.3230e-02,\n",
      "           1.0012e-01, 5.3883e-01],\n",
      "          [1.5880e-01, 5.0026e-01, 2.7876e-02,  ..., 3.6283e-03,\n",
      "           4.3101e-02, 2.1972e-01],\n",
      "          ...,\n",
      "          [7.7547e-03, 1.6340e-03, 2.0719e-03,  ..., 7.6910e-03,\n",
      "           1.0528e-02, 4.4503e-01],\n",
      "          [2.0382e-02, 3.0774e-03, 6.4983e-04,  ..., 2.9660e-01,\n",
      "           1.2091e-01, 4.1620e-01],\n",
      "          [1.4667e-02, 2.3370e-03, 2.0848e-03,  ..., 3.8336e-03,\n",
      "           1.3663e-02, 9.0372e-01]],\n",
      "\n",
      "         [[4.7107e-02, 1.1229e-02, 1.1511e-02,  ..., 2.9370e-03,\n",
      "           4.2227e-02, 8.0258e-01],\n",
      "          [7.6179e-02, 2.7886e-02, 1.8984e-01,  ..., 4.6590e-03,\n",
      "           5.9423e-02, 3.2369e-01],\n",
      "          [1.2037e-01, 1.4213e-02, 1.5067e-02,  ..., 4.3184e-03,\n",
      "           3.6187e-02, 3.6775e-01],\n",
      "          ...,\n",
      "          [2.8183e-02, 4.8642e-03, 2.1589e-03,  ..., 3.4186e-02,\n",
      "           2.2515e-02, 8.1388e-01],\n",
      "          [1.0099e-01, 1.7568e-02, 1.3145e-02,  ..., 1.8628e-03,\n",
      "           8.2253e-02, 6.6433e-01],\n",
      "          [1.4205e-02, 6.7812e-03, 3.2565e-03,  ..., 5.0518e-03,\n",
      "           9.3748e-03, 9.0743e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.8974e-03, 5.6182e-02, 1.3736e-01,  ..., 5.5423e-02,\n",
      "           1.6889e-02, 1.6178e-02],\n",
      "          [3.0771e-03, 6.8337e-03, 5.1509e-03,  ..., 1.8149e-03,\n",
      "           1.3980e-03, 9.6022e-01],\n",
      "          [1.5974e-03, 4.1406e-03, 3.2511e-02,  ..., 8.2177e-04,\n",
      "           7.6291e-04, 9.1899e-01],\n",
      "          ...,\n",
      "          [5.5021e-03, 6.9110e-03, 9.5305e-03,  ..., 8.5421e-02,\n",
      "           4.0988e-03, 7.8954e-01],\n",
      "          [1.8740e-02, 5.0311e-02, 1.6249e-01,  ..., 6.2426e-02,\n",
      "           3.8180e-02, 2.3745e-02],\n",
      "          [3.3150e-03, 6.5254e-04, 1.7417e-03,  ..., 1.6416e-03,\n",
      "           3.1781e-03, 9.7736e-01]],\n",
      "\n",
      "         [[3.7214e-02, 4.9456e-02, 4.2415e-01,  ..., 3.5178e-03,\n",
      "           4.3116e-02, 5.8105e-02],\n",
      "          [1.2419e-01, 7.7922e-02, 6.3776e-02,  ..., 2.3873e-02,\n",
      "           6.2347e-02, 3.9866e-01],\n",
      "          [8.0357e-02, 1.8229e-01, 7.9189e-02,  ..., 2.1774e-02,\n",
      "           4.3992e-02, 4.2533e-01],\n",
      "          ...,\n",
      "          [8.0626e-03, 8.6999e-03, 3.7348e-02,  ..., 4.7164e-02,\n",
      "           5.6949e-03, 7.0135e-01],\n",
      "          [3.0192e-02, 1.2245e-01, 1.8582e-01,  ..., 7.7083e-03,\n",
      "           3.8886e-02, 6.4524e-02],\n",
      "          [8.5138e-03, 1.8591e-03, 3.9524e-03,  ..., 9.9037e-03,\n",
      "           7.7846e-03, 9.3913e-01]],\n",
      "\n",
      "         [[5.4785e-03, 5.5560e-03, 4.1502e-03,  ..., 9.6012e-03,\n",
      "           7.9311e-01, 4.9505e-02],\n",
      "          [1.7415e-02, 2.2287e-02, 7.0430e-03,  ..., 2.4763e-03,\n",
      "           1.8919e-02, 8.9815e-01],\n",
      "          [5.0077e-02, 3.5116e-02, 2.3817e-02,  ..., 1.4704e-03,\n",
      "           3.3627e-02, 8.3626e-01],\n",
      "          ...,\n",
      "          [1.7466e-02, 2.2776e-03, 5.3758e-03,  ..., 2.6333e-02,\n",
      "           2.5478e-02, 2.7430e-01],\n",
      "          [2.4614e-02, 1.2935e-02, 1.7019e-02,  ..., 7.2754e-02,\n",
      "           8.2484e-02, 1.4675e-01],\n",
      "          [1.8682e-02, 2.7100e-03, 5.4457e-03,  ..., 4.1657e-03,\n",
      "           3.2982e-02, 8.8691e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[8.8069e-02, 3.7844e-03, 3.5629e-03,  ..., 4.2042e-03,\n",
      "           1.3338e-01, 7.2061e-01],\n",
      "          [1.2358e-01, 3.4610e-02, 9.0105e-03,  ..., 1.7158e-03,\n",
      "           1.5176e-01, 6.6703e-01],\n",
      "          [1.2700e-01, 3.1568e-01, 1.2694e-02,  ..., 1.9521e-03,\n",
      "           7.6380e-02, 4.5317e-01],\n",
      "          ...,\n",
      "          [8.9680e-03, 2.2011e-03, 1.5929e-03,  ..., 1.8562e-02,\n",
      "           8.5831e-03, 7.9260e-02],\n",
      "          [2.5885e-02, 1.0754e-03, 1.1075e-03,  ..., 2.8067e-02,\n",
      "           5.5024e-02, 8.2751e-01],\n",
      "          [1.4036e-02, 3.0779e-03, 6.4202e-03,  ..., 4.3380e-03,\n",
      "           2.2444e-02, 8.5985e-01]],\n",
      "\n",
      "         [[7.9782e-03, 2.1210e-03, 4.1502e-03,  ..., 8.7340e-02,\n",
      "           1.2755e-02, 5.3324e-01],\n",
      "          [3.4986e-03, 1.3717e-02, 9.3128e-04,  ..., 5.3721e-03,\n",
      "           6.1093e-03, 9.3013e-01],\n",
      "          [3.8589e-03, 9.2372e-02, 7.5764e-03,  ..., 1.6220e-03,\n",
      "           5.8069e-03, 8.6144e-01],\n",
      "          ...,\n",
      "          [8.5134e-03, 1.1607e-03, 1.7086e-03,  ..., 5.4503e-02,\n",
      "           1.1724e-02, 5.0208e-01],\n",
      "          [2.5372e-02, 3.7140e-03, 1.1152e-03,  ..., 3.2286e-02,\n",
      "           3.6207e-02, 5.0144e-01],\n",
      "          [6.6862e-03, 2.6945e-03, 3.6115e-03,  ..., 5.3187e-03,\n",
      "           1.0147e-02, 8.8733e-01]],\n",
      "\n",
      "         [[5.6620e-03, 5.2820e-03, 2.8612e-03,  ..., 1.4593e-02,\n",
      "           7.6074e-01, 8.4306e-02],\n",
      "          [2.4396e-01, 2.2040e-02, 1.7740e-02,  ..., 6.2471e-03,\n",
      "           2.7807e-01, 3.5951e-01],\n",
      "          [1.9292e-02, 8.3747e-01, 5.8279e-03,  ..., 1.1501e-03,\n",
      "           5.4165e-03, 8.1600e-02],\n",
      "          ...,\n",
      "          [1.0466e-03, 2.2466e-04, 7.5549e-05,  ..., 4.4180e-03,\n",
      "           2.3446e-03, 8.5314e-01],\n",
      "          [1.3151e-03, 5.4246e-04, 1.1728e-03,  ..., 9.2199e-02,\n",
      "           1.1909e-01, 7.2874e-01],\n",
      "          [3.9503e-03, 3.7502e-03, 2.3323e-03,  ..., 6.9795e-03,\n",
      "           9.8804e-03, 8.8520e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.8653e-03, 1.0042e-02, 1.4826e-02,  ..., 6.3413e-02,\n",
      "           3.5771e-02, 6.8576e-01],\n",
      "          [1.9833e-02, 1.8012e-02, 2.1934e-02,  ..., 5.5013e-03,\n",
      "           2.5760e-02, 8.7969e-01],\n",
      "          [2.7328e-02, 6.9032e-02, 6.3091e-02,  ..., 3.3878e-03,\n",
      "           3.5933e-02, 7.4157e-01],\n",
      "          ...,\n",
      "          [4.3154e-03, 1.8560e-02, 4.1293e-02,  ..., 7.0083e-03,\n",
      "           9.9969e-03, 4.5227e-02],\n",
      "          [8.9592e-03, 2.6295e-02, 2.1040e-02,  ..., 7.8784e-02,\n",
      "           8.4670e-02, 1.9881e-01],\n",
      "          [1.0007e-02, 1.8786e-03, 3.0194e-03,  ..., 3.7474e-03,\n",
      "           2.5315e-02, 9.3190e-01]],\n",
      "\n",
      "         [[9.7526e-02, 4.3710e-02, 5.9676e-02,  ..., 1.4773e-02,\n",
      "           7.6433e-02, 4.0575e-01],\n",
      "          [1.6435e-02, 8.0701e-02, 3.2826e-01,  ..., 6.2244e-03,\n",
      "           6.1691e-03, 1.7692e-01],\n",
      "          [5.2178e-03, 6.6279e-03, 2.1418e-02,  ..., 4.5930e-03,\n",
      "           5.9443e-03, 6.3620e-02],\n",
      "          ...,\n",
      "          [1.2724e-02, 4.4631e-03, 2.8298e-03,  ..., 2.2242e-02,\n",
      "           2.4044e-02, 8.1825e-01],\n",
      "          [6.3296e-02, 5.7301e-02, 5.5008e-02,  ..., 1.5688e-02,\n",
      "           6.4185e-02, 4.3674e-01],\n",
      "          [1.0384e-02, 1.5897e-03, 1.3787e-03,  ..., 4.6086e-03,\n",
      "           5.4031e-03, 9.4713e-01]],\n",
      "\n",
      "         [[1.0938e-01, 3.7669e-02, 9.4600e-03,  ..., 6.3347e-03,\n",
      "           2.9000e-02, 7.3153e-01],\n",
      "          [3.3769e-03, 1.5215e-02, 7.9501e-01,  ..., 1.4123e-03,\n",
      "           3.8211e-04, 1.4881e-01],\n",
      "          [4.2000e-03, 3.4384e-02, 7.3983e-02,  ..., 2.3080e-03,\n",
      "           9.4117e-04, 5.9113e-01],\n",
      "          ...,\n",
      "          [5.7510e-02, 1.3145e-03, 1.1228e-03,  ..., 1.7276e-02,\n",
      "           3.4304e-01, 4.6141e-01],\n",
      "          [1.6050e-01, 9.0537e-02, 9.9840e-03,  ..., 7.9029e-03,\n",
      "           2.8897e-02, 6.0227e-01],\n",
      "          [2.7062e-02, 8.0053e-03, 5.0440e-03,  ..., 9.6832e-03,\n",
      "           2.1725e-02, 8.3740e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.7170e-03, 1.8025e-03, 2.8895e-03,  ..., 2.0104e-02,\n",
      "           1.5260e-02, 9.0284e-01],\n",
      "          [2.5244e-02, 7.4032e-03, 7.7648e-03,  ..., 5.3236e-03,\n",
      "           6.2643e-02, 8.7281e-01],\n",
      "          [3.5476e-02, 3.2005e-02, 1.8361e-02,  ..., 5.0241e-03,\n",
      "           5.4267e-02, 8.3305e-01],\n",
      "          ...,\n",
      "          [9.0093e-03, 1.4795e-03, 2.3728e-03,  ..., 1.2076e-02,\n",
      "           4.3719e-03, 1.4783e-01],\n",
      "          [1.1769e-02, 1.2845e-03, 2.9154e-03,  ..., 2.4271e-02,\n",
      "           5.9128e-02, 7.7684e-01],\n",
      "          [1.5738e-02, 3.1777e-03, 5.2268e-03,  ..., 8.9945e-03,\n",
      "           3.1458e-02, 8.6822e-01]],\n",
      "\n",
      "         [[4.0019e-02, 2.0078e-03, 3.5124e-02,  ..., 1.8695e-02,\n",
      "           1.5424e-02, 6.5800e-01],\n",
      "          [8.8078e-03, 5.7749e-03, 6.3303e-01,  ..., 1.7140e-03,\n",
      "           1.0594e-03, 1.7992e-01],\n",
      "          [5.9677e-03, 6.8349e-03, 1.4326e-01,  ..., 1.8588e-03,\n",
      "           1.1594e-03, 3.8818e-01],\n",
      "          ...,\n",
      "          [9.8241e-03, 1.8170e-03, 3.6777e-03,  ..., 6.7150e-02,\n",
      "           3.1094e-03, 7.4080e-01],\n",
      "          [9.3371e-02, 7.7209e-03, 6.9957e-02,  ..., 2.3862e-02,\n",
      "           4.2535e-02, 5.0581e-01],\n",
      "          [2.3865e-02, 1.4325e-03, 2.7256e-03,  ..., 4.0213e-03,\n",
      "           1.2219e-02, 9.2580e-01]],\n",
      "\n",
      "         [[2.4865e-03, 8.6289e-04, 1.2070e-03,  ..., 5.8679e-03,\n",
      "           9.1670e-01, 5.3138e-02],\n",
      "          [2.8177e-02, 9.6293e-03, 2.2204e-03,  ..., 6.9543e-03,\n",
      "           3.3485e-02, 8.8624e-01],\n",
      "          [1.7185e-02, 1.1543e-01, 1.7139e-02,  ..., 6.0654e-03,\n",
      "           1.0875e-02, 7.9498e-01],\n",
      "          ...,\n",
      "          [6.4837e-03, 2.6670e-04, 6.8818e-04,  ..., 1.9462e-02,\n",
      "           1.1233e-02, 9.1570e-01],\n",
      "          [3.5331e-02, 8.1777e-03, 1.8847e-03,  ..., 1.1165e-01,\n",
      "           9.8374e-02, 5.8918e-01],\n",
      "          [1.6393e-02, 1.7171e-03, 1.0830e-03,  ..., 9.2994e-03,\n",
      "           4.7599e-02, 8.8280e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[8.0743e-02, 1.2118e-02, 4.1022e-02,  ..., 4.2910e-03,\n",
      "           4.1766e-02, 5.5640e-01],\n",
      "          [9.7454e-02, 3.5804e-02, 6.6927e-02,  ..., 6.1308e-03,\n",
      "           2.5808e-02, 3.5227e-01],\n",
      "          [4.8327e-02, 4.5608e-02, 3.6676e-02,  ..., 2.2483e-03,\n",
      "           7.5929e-03, 3.1887e-01],\n",
      "          ...,\n",
      "          [2.6596e-01, 9.7074e-04, 7.8350e-04,  ..., 1.9215e-02,\n",
      "           3.8029e-02, 6.1462e-01],\n",
      "          [1.3973e-01, 7.1694e-03, 3.4940e-02,  ..., 8.1116e-03,\n",
      "           4.7718e-02, 5.4372e-01],\n",
      "          [3.0966e-02, 2.3013e-03, 2.6816e-03,  ..., 2.5587e-03,\n",
      "           7.7677e-03, 9.2476e-01]],\n",
      "\n",
      "         [[6.4219e-02, 4.2106e-02, 1.8518e-02,  ..., 1.3403e-02,\n",
      "           4.6507e-02, 5.1546e-01],\n",
      "          [1.0150e-01, 3.3520e-02, 7.3617e-03,  ..., 1.5462e-02,\n",
      "           1.3372e-01, 4.1592e-01],\n",
      "          [3.2724e-02, 2.7502e-02, 1.0002e-02,  ..., 4.6468e-02,\n",
      "           4.1706e-02, 5.6616e-01],\n",
      "          ...,\n",
      "          [9.6328e-03, 6.1754e-03, 3.7419e-03,  ..., 4.0058e-02,\n",
      "           2.2923e-02, 7.9419e-01],\n",
      "          [9.8857e-02, 2.1905e-02, 1.9828e-02,  ..., 1.7779e-02,\n",
      "           6.2843e-02, 3.8426e-01],\n",
      "          [1.3820e-02, 1.2243e-02, 1.2422e-02,  ..., 2.0505e-02,\n",
      "           6.7762e-02, 7.4669e-01]],\n",
      "\n",
      "         [[1.5607e-02, 3.2266e-02, 3.9323e-03,  ..., 9.7481e-03,\n",
      "           1.1609e-02, 8.7816e-01],\n",
      "          [4.0986e-03, 2.8610e-02, 4.6821e-01,  ..., 6.4511e-03,\n",
      "           4.4858e-03, 4.5469e-01],\n",
      "          [4.0156e-03, 3.1699e-02, 1.2848e-02,  ..., 2.9082e-03,\n",
      "           2.2084e-03, 8.6922e-01],\n",
      "          ...,\n",
      "          [7.7948e-03, 5.1069e-04, 5.6265e-04,  ..., 3.1985e-03,\n",
      "           6.0918e-03, 9.6220e-01],\n",
      "          [6.4775e-03, 2.1010e-01, 1.2901e-02,  ..., 4.5190e-03,\n",
      "           3.3405e-03, 6.6404e-01],\n",
      "          [1.6875e-02, 1.4229e-02, 6.6317e-03,  ..., 1.6879e-02,\n",
      "           1.1891e-02, 8.4712e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.2201e-02, 2.9062e-03, 1.7147e-02,  ..., 2.5945e-02,\n",
      "           2.1658e-02, 7.9498e-01],\n",
      "          [6.0278e-02, 1.9443e-02, 1.7666e-01,  ..., 3.4333e-02,\n",
      "           7.0787e-02, 3.3002e-01],\n",
      "          [4.7420e-02, 6.9615e-03, 3.1287e-02,  ..., 2.7692e-02,\n",
      "           7.5282e-02, 1.7779e-01],\n",
      "          ...,\n",
      "          [1.4608e-01, 2.7349e-03, 3.6963e-03,  ..., 2.3829e-02,\n",
      "           4.0591e-01, 2.1882e-01],\n",
      "          [5.1383e-02, 6.2343e-03, 2.0406e-02,  ..., 1.9599e-02,\n",
      "           2.2726e-02, 7.5433e-01],\n",
      "          [2.2733e-02, 2.8110e-03, 4.2512e-03,  ..., 1.4097e-02,\n",
      "           2.4984e-02, 8.6942e-01]],\n",
      "\n",
      "         [[7.2207e-03, 2.5448e-03, 1.3641e-03,  ..., 1.3956e-02,\n",
      "           1.2483e-02, 8.8019e-01],\n",
      "          [5.9567e-03, 7.0452e-03, 2.5075e-02,  ..., 1.5207e-02,\n",
      "           1.2111e-02, 8.4840e-01],\n",
      "          [8.9485e-03, 4.5865e-02, 5.5530e-02,  ..., 1.7128e-02,\n",
      "           9.3726e-03, 4.5506e-01],\n",
      "          ...,\n",
      "          [1.1385e-02, 1.1594e-02, 2.6268e-02,  ..., 5.3495e-02,\n",
      "           2.1910e-02, 4.9841e-01],\n",
      "          [5.1438e-03, 4.8396e-03, 4.8449e-03,  ..., 4.5463e-02,\n",
      "           1.6170e-02, 7.4791e-01],\n",
      "          [3.2706e-03, 7.8017e-04, 1.3832e-03,  ..., 6.5816e-03,\n",
      "           5.9954e-03, 9.5522e-01]],\n",
      "\n",
      "         [[3.2438e-02, 3.9292e-03, 3.4071e-03,  ..., 5.2580e-03,\n",
      "           7.6779e-02, 8.4406e-01],\n",
      "          [2.2463e-02, 4.9077e-02, 8.9854e-02,  ..., 1.0586e-03,\n",
      "           1.1188e-01, 6.6234e-01],\n",
      "          [3.0571e-02, 1.0949e-01, 9.0993e-02,  ..., 3.0010e-03,\n",
      "           7.7337e-02, 5.7035e-01],\n",
      "          ...,\n",
      "          [2.1709e-02, 7.5487e-03, 5.3303e-02,  ..., 1.2960e-02,\n",
      "           1.5226e-02, 9.9098e-02],\n",
      "          [3.6200e-02, 2.3645e-02, 2.4740e-02,  ..., 6.4403e-03,\n",
      "           9.1787e-02, 7.4095e-01],\n",
      "          [1.5024e-02, 7.0702e-03, 4.9657e-03,  ..., 7.0266e-03,\n",
      "           2.7662e-02, 8.7457e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.0280e-02, 1.6849e-01, 1.2754e-01,  ..., 3.1687e-02,\n",
      "           3.0569e-02, 2.9409e-01],\n",
      "          [1.2974e-02, 1.3250e-01, 1.8912e-02,  ..., 5.1667e-02,\n",
      "           2.4952e-02, 6.2905e-01],\n",
      "          [7.1869e-03, 7.0668e-02, 1.8913e-01,  ..., 4.7662e-02,\n",
      "           1.0319e-02, 5.9144e-01],\n",
      "          ...,\n",
      "          [3.6352e-03, 1.9952e-03, 8.9563e-04,  ..., 2.8818e-01,\n",
      "           6.3338e-03, 6.4335e-01],\n",
      "          [1.0968e-02, 1.1853e-02, 4.4101e-03,  ..., 9.2092e-03,\n",
      "           4.7096e-03, 8.5813e-01],\n",
      "          [5.0173e-03, 2.3975e-02, 5.3284e-03,  ..., 2.3199e-02,\n",
      "           7.8596e-04, 8.1436e-01]],\n",
      "\n",
      "         [[4.9615e-02, 7.7723e-03, 4.6570e-03,  ..., 3.3852e-03,\n",
      "           4.7386e-02, 7.8940e-01],\n",
      "          [2.2526e-01, 8.7546e-03, 3.3272e-03,  ..., 4.2524e-03,\n",
      "           1.3036e-02, 7.1282e-01],\n",
      "          [1.0628e-01, 1.8477e-02, 9.1871e-03,  ..., 8.8959e-03,\n",
      "           3.3533e-02, 5.9185e-01],\n",
      "          ...,\n",
      "          [1.7631e-01, 2.7698e-02, 7.9101e-03,  ..., 6.4703e-02,\n",
      "           5.3456e-02, 5.1037e-01],\n",
      "          [1.4106e-01, 1.4948e-02, 1.5583e-02,  ..., 1.8969e-02,\n",
      "           7.3268e-03, 6.8985e-01],\n",
      "          [1.3244e-01, 1.4309e-02, 6.1105e-03,  ..., 1.9222e-02,\n",
      "           1.9525e-02, 6.8397e-01]],\n",
      "\n",
      "         [[5.1883e-02, 2.0399e-02, 3.1677e-02,  ..., 7.1779e-03,\n",
      "           8.0521e-02, 6.9034e-01],\n",
      "          [7.1233e-02, 2.3935e-02, 2.8238e-02,  ..., 7.9021e-03,\n",
      "           4.7362e-02, 4.3159e-01],\n",
      "          [1.9644e-01, 3.7586e-02, 1.9255e-02,  ..., 4.5100e-03,\n",
      "           9.0610e-02, 4.6949e-01],\n",
      "          ...,\n",
      "          [4.2183e-02, 1.3863e-02, 8.7663e-03,  ..., 8.1427e-02,\n",
      "           2.1724e-02, 6.5099e-01],\n",
      "          [2.8523e-02, 2.7317e-02, 2.0080e-02,  ..., 2.1292e-02,\n",
      "           2.6571e-02, 6.8962e-01],\n",
      "          [2.9679e-02, 3.8499e-02, 4.2203e-02,  ..., 1.2724e-02,\n",
      "           3.5423e-02, 4.9476e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.8625e-02, 8.4832e-03, 1.8230e-03,  ..., 3.1481e-02,\n",
      "           8.5084e-03, 8.1618e-01],\n",
      "          [3.7123e-02, 1.6879e-02, 6.3573e-03,  ..., 3.1290e-03,\n",
      "           1.2668e-02, 7.7284e-01],\n",
      "          [1.1252e-02, 5.7467e-02, 6.8032e-03,  ..., 1.0760e-03,\n",
      "           7.6927e-03, 8.0923e-01],\n",
      "          ...,\n",
      "          [2.0378e-02, 1.4922e-02, 2.8322e-03,  ..., 2.4244e-02,\n",
      "           1.7796e-02, 7.6790e-01],\n",
      "          [1.8129e-02, 7.8561e-03, 4.6014e-03,  ..., 3.1428e-02,\n",
      "           1.9928e-02, 7.9866e-01],\n",
      "          [5.1708e-02, 1.7333e-02, 8.6523e-03,  ..., 2.8201e-02,\n",
      "           4.5672e-02, 6.1841e-01]],\n",
      "\n",
      "         [[2.8531e-02, 4.1969e-02, 3.1624e-02,  ..., 6.1499e-02,\n",
      "           4.0621e-02, 2.5692e-01],\n",
      "          [7.0785e-02, 3.2506e-02, 2.5521e-02,  ..., 4.3944e-02,\n",
      "           2.0254e-02, 6.0616e-02],\n",
      "          [9.0265e-02, 1.0805e-02, 1.0378e-02,  ..., 1.8517e-02,\n",
      "           3.5544e-02, 1.0134e-01],\n",
      "          ...,\n",
      "          [2.4044e-01, 1.8022e-02, 3.6341e-03,  ..., 7.8992e-02,\n",
      "           2.2108e-02, 4.8847e-01],\n",
      "          [1.2820e-01, 5.1823e-03, 2.0096e-03,  ..., 8.5713e-03,\n",
      "           9.5299e-03, 7.9743e-01],\n",
      "          [4.3951e-02, 5.0250e-02, 1.6751e-02,  ..., 3.8666e-02,\n",
      "           5.9539e-03, 1.8061e-01]],\n",
      "\n",
      "         [[1.0160e-01, 4.4456e-02, 7.2974e-02,  ..., 8.1140e-02,\n",
      "           6.5037e-02, 1.7389e-01],\n",
      "          [1.6090e-01, 2.5096e-02, 1.5876e-02,  ..., 1.7770e-02,\n",
      "           6.7620e-02, 5.3567e-01],\n",
      "          [8.3279e-02, 1.0042e-02, 2.1206e-03,  ..., 3.2818e-02,\n",
      "           3.7621e-02, 7.2358e-01],\n",
      "          ...,\n",
      "          [7.9399e-02, 4.3544e-03, 2.0792e-02,  ..., 2.2157e-03,\n",
      "           3.6006e-02, 4.6222e-01],\n",
      "          [5.9342e-02, 3.2127e-02, 1.7846e-02,  ..., 5.2797e-02,\n",
      "           8.8678e-02, 4.4536e-01],\n",
      "          [8.5482e-02, 5.2538e-02, 4.7970e-02,  ..., 5.3558e-02,\n",
      "           1.1445e-01, 6.9286e-02]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.5267e-02, 1.4077e-01, 5.8067e-02,  ..., 2.5298e-02,\n",
      "           1.0925e-01, 1.5567e-01],\n",
      "          [4.1152e-02, 1.4967e-01, 1.4241e-02,  ..., 2.6568e-02,\n",
      "           1.4739e-01, 2.2675e-01],\n",
      "          [4.9057e-02, 9.8784e-02, 3.9559e-02,  ..., 1.3971e-02,\n",
      "           2.8720e-01, 2.8731e-01],\n",
      "          ...,\n",
      "          [1.3595e-02, 3.5318e-03, 1.9547e-03,  ..., 5.0428e-02,\n",
      "           6.4430e-01, 2.3900e-01],\n",
      "          [2.0408e-02, 1.2882e-02, 1.3560e-02,  ..., 4.0560e-02,\n",
      "           4.0373e-01, 2.7219e-01],\n",
      "          [1.9570e-02, 1.3245e-02, 8.8292e-03,  ..., 2.7708e-02,\n",
      "           4.6229e-01, 3.2782e-01]],\n",
      "\n",
      "         [[2.5634e-02, 9.7853e-03, 5.6505e-02,  ..., 1.3379e-02,\n",
      "           4.9817e-01, 2.8052e-01],\n",
      "          [2.5966e-03, 4.1782e-03, 2.0967e-03,  ..., 3.0026e-03,\n",
      "           6.4815e-01, 2.9966e-01],\n",
      "          [2.5092e-03, 1.8500e-03, 5.2749e-02,  ..., 4.7736e-03,\n",
      "           6.8332e-01, 2.2440e-01],\n",
      "          ...,\n",
      "          [5.7747e-04, 1.6643e-04, 7.4950e-04,  ..., 3.5998e-02,\n",
      "           7.4395e-01, 2.0685e-01],\n",
      "          [4.3267e-03, 7.0035e-04, 5.1701e-04,  ..., 5.7722e-04,\n",
      "           7.2093e-01, 2.5882e-01],\n",
      "          [2.7638e-03, 5.8481e-04, 3.1922e-04,  ..., 4.5901e-04,\n",
      "           7.0403e-01, 2.8018e-01]],\n",
      "\n",
      "         [[1.8041e-02, 2.7103e-02, 2.2917e-02,  ..., 1.7534e-02,\n",
      "           4.3584e-01, 3.0500e-01],\n",
      "          [5.7785e-02, 6.0532e-02, 7.1321e-02,  ..., 2.9399e-02,\n",
      "           3.0602e-01, 2.8317e-01],\n",
      "          [2.8158e-02, 3.2892e-02, 7.9782e-02,  ..., 6.8229e-03,\n",
      "           4.8423e-01, 2.6816e-01],\n",
      "          ...,\n",
      "          [3.5716e-02, 2.2605e-02, 7.3218e-03,  ..., 1.8763e-01,\n",
      "           1.1573e-01, 5.6112e-02],\n",
      "          [2.7023e-02, 1.2057e-02, 8.5038e-03,  ..., 8.5378e-03,\n",
      "           5.6261e-01, 2.6774e-01],\n",
      "          [3.2664e-02, 1.3810e-02, 9.3209e-03,  ..., 7.4539e-03,\n",
      "           5.0150e-01, 3.2674e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.9344e-02, 2.2112e-02, 3.9165e-03,  ..., 7.3290e-03,\n",
      "           4.8463e-01, 3.4679e-01],\n",
      "          [2.8620e-03, 9.2823e-02, 6.5571e-03,  ..., 2.0173e-03,\n",
      "           5.2987e-01, 3.1541e-01],\n",
      "          [4.5429e-03, 1.2820e-02, 1.1378e-01,  ..., 5.2645e-03,\n",
      "           5.8052e-01, 2.5285e-01],\n",
      "          ...,\n",
      "          [2.0924e-03, 7.2355e-04, 2.8151e-03,  ..., 3.8023e-02,\n",
      "           6.3173e-01, 2.9231e-01],\n",
      "          [1.4159e-03, 1.0636e-03, 1.1057e-03,  ..., 1.7570e-03,\n",
      "           7.5124e-01, 2.2341e-01],\n",
      "          [1.5086e-03, 1.2516e-03, 9.3495e-04,  ..., 1.5738e-03,\n",
      "           7.3665e-01, 2.4051e-01]],\n",
      "\n",
      "         [[3.8021e-02, 6.9216e-03, 3.0108e-02,  ..., 3.1587e-03,\n",
      "           5.4106e-01, 2.6079e-01],\n",
      "          [6.7605e-01, 1.1087e-01, 2.6644e-02,  ..., 3.1666e-04,\n",
      "           1.5291e-02, 1.1192e-01],\n",
      "          [7.1905e-01, 5.9133e-03, 1.6552e-01,  ..., 1.7445e-05,\n",
      "           1.0470e-02, 6.7224e-02],\n",
      "          ...,\n",
      "          [1.4776e-02, 1.8803e-05, 3.5189e-06,  ..., 9.5830e-01,\n",
      "           9.0583e-04, 5.9099e-03],\n",
      "          [7.3987e-02, 4.4234e-02, 3.8523e-02,  ..., 5.6298e-02,\n",
      "           1.5518e-01, 1.3242e-01],\n",
      "          [1.2315e-01, 4.5676e-02, 3.8850e-02,  ..., 3.7059e-02,\n",
      "           1.8608e-01, 1.9240e-01]],\n",
      "\n",
      "         [[5.1634e-02, 2.9605e-02, 5.9856e-02,  ..., 5.5743e-02,\n",
      "           9.9313e-02, 8.6445e-02],\n",
      "          [1.9064e-03, 6.7228e-03, 3.3101e-02,  ..., 7.7699e-02,\n",
      "           2.4767e-02, 1.7558e-02],\n",
      "          [1.7107e-03, 6.6288e-03, 2.6363e-02,  ..., 3.0815e-02,\n",
      "           1.1726e-01, 6.3288e-02],\n",
      "          ...,\n",
      "          [4.9503e-03, 6.2483e-03, 8.4804e-03,  ..., 1.2918e-01,\n",
      "           4.2425e-01, 1.2445e-01],\n",
      "          [8.6377e-02, 8.1522e-03, 6.5221e-03,  ..., 1.0260e-02,\n",
      "           5.8293e-01, 2.5180e-01],\n",
      "          [4.9661e-02, 8.9593e-03, 6.4858e-03,  ..., 1.3868e-02,\n",
      "           5.8066e-01, 2.6247e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.0775, 0.0116, 0.0200,  ..., 0.0091, 0.2763, 0.2598],\n",
      "          [0.0056, 0.0008, 0.0013,  ..., 0.0048, 0.5090, 0.4331],\n",
      "          [0.0119, 0.0011, 0.0022,  ..., 0.0071, 0.4824, 0.4212],\n",
      "          ...,\n",
      "          [0.0113, 0.0006, 0.0010,  ..., 0.0078, 0.5139, 0.4272],\n",
      "          [0.0062, 0.0026, 0.0024,  ..., 0.0008, 0.5264, 0.4360],\n",
      "          [0.0057, 0.0023, 0.0021,  ..., 0.0007, 0.5292, 0.4370]],\n",
      "\n",
      "         [[0.2196, 0.0213, 0.0147,  ..., 0.0185, 0.2160, 0.2084],\n",
      "          [0.0139, 0.0034, 0.0036,  ..., 0.0014, 0.5098, 0.4468],\n",
      "          [0.0210, 0.0056, 0.0056,  ..., 0.0012, 0.4960, 0.4390],\n",
      "          ...,\n",
      "          [0.0140, 0.0088, 0.0110,  ..., 0.0394, 0.4088, 0.3537],\n",
      "          [0.0087, 0.0072, 0.0056,  ..., 0.0024, 0.4997, 0.4367],\n",
      "          [0.0082, 0.0064, 0.0049,  ..., 0.0021, 0.5039, 0.4393]],\n",
      "\n",
      "         [[0.0769, 0.0563, 0.0627,  ..., 0.0256, 0.0262, 0.0276],\n",
      "          [0.0133, 0.0243, 0.0480,  ..., 0.0235, 0.1628, 0.1584],\n",
      "          [0.0120, 0.0120, 0.0584,  ..., 0.0079, 0.2472, 0.2392],\n",
      "          ...,\n",
      "          [0.0160, 0.0127, 0.0148,  ..., 0.1479, 0.3122, 0.2855],\n",
      "          [0.0127, 0.0038, 0.0063,  ..., 0.0046, 0.4794, 0.4289],\n",
      "          [0.0113, 0.0036, 0.0060,  ..., 0.0043, 0.4824, 0.4313]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0772, 0.0403, 0.0815,  ..., 0.0359, 0.0061, 0.0062],\n",
      "          [0.0486, 0.0541, 0.0791,  ..., 0.0482, 0.0580, 0.0551],\n",
      "          [0.0622, 0.0346, 0.0763,  ..., 0.0143, 0.2179, 0.2037],\n",
      "          ...,\n",
      "          [0.0531, 0.0140, 0.0163,  ..., 0.1243, 0.1992, 0.1895],\n",
      "          [0.0183, 0.0061, 0.0078,  ..., 0.0138, 0.4783, 0.4169],\n",
      "          [0.0186, 0.0060, 0.0077,  ..., 0.0136, 0.4790, 0.4176]],\n",
      "\n",
      "         [[0.0338, 0.0654, 0.0504,  ..., 0.0168, 0.1315, 0.1392],\n",
      "          [0.0118, 0.0040, 0.0048,  ..., 0.0041, 0.4841, 0.4313],\n",
      "          [0.0288, 0.0082, 0.0177,  ..., 0.0081, 0.4236, 0.3799],\n",
      "          ...,\n",
      "          [0.0071, 0.0050, 0.0047,  ..., 0.0470, 0.4615, 0.3885],\n",
      "          [0.0065, 0.0028, 0.0036,  ..., 0.0031, 0.5131, 0.4359],\n",
      "          [0.0061, 0.0026, 0.0033,  ..., 0.0028, 0.5150, 0.4374]],\n",
      "\n",
      "         [[0.0123, 0.0116, 0.0124,  ..., 0.0127, 0.2995, 0.2924],\n",
      "          [0.0055, 0.0024, 0.0029,  ..., 0.0015, 0.5084, 0.4285],\n",
      "          [0.0092, 0.0086, 0.0231,  ..., 0.0054, 0.4181, 0.3450],\n",
      "          ...,\n",
      "          [0.0124, 0.0061, 0.0099,  ..., 0.0481, 0.3348, 0.2855],\n",
      "          [0.0020, 0.0013, 0.0012,  ..., 0.0040, 0.5283, 0.4323],\n",
      "          [0.0019, 0.0012, 0.0011,  ..., 0.0036, 0.5296, 0.4333]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)), cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# tokenizer, using library and pretrained embeddings\n",
    "lib_token = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "lib_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
    "\n",
    "inputt = lib_token(example, return_tensors=\"pt\")\n",
    "print(\"library input ids: \", inputt['input_ids'])\n",
    "out = lib_model(**inputt)\n",
    "print(\"library output last hidden state: \", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b48203c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library tokens:  ['[CLS]', 'i', 'think', 'the', 'transformers', 'movie', 'is', 'great', 'and', 'it', 'reminds', 'me', 'of', 'my', 'childhood', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = lib_token.convert_ids_to_tokens(inputt[\"input_ids\"][0])\n",
    "print(\"library tokens: \", tokens)\n",
    "attention = out.attentions[-1].mean(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78cb65ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                [CLS]        i    think      the  transformers    movie  \\\n",
      "[CLS]         0.06400  0.02882  0.03441  0.07551       0.06283  0.04804   \n",
      "i             0.01872  0.01916  0.01837  0.02005       0.01023  0.00992   \n",
      "think         0.02251  0.01419  0.05490  0.01747       0.00470  0.00725   \n",
      "the           0.02309  0.00729  0.01147  0.05147       0.01790  0.02902   \n",
      "transformers  0.02872  0.01053  0.01125  0.02388       0.11720  0.03879   \n",
      "movie         0.01925  0.00570  0.00730  0.02872       0.04509  0.10236   \n",
      "is            0.01733  0.00704  0.01370  0.02586       0.00943  0.01584   \n",
      "great         0.02111  0.00637  0.01341  0.02193       0.00921  0.01373   \n",
      "and           0.03192  0.01238  0.01556  0.01084       0.01127  0.00950   \n",
      "it            0.01728  0.00620  0.01001  0.02268       0.01444  0.01851   \n",
      "reminds       0.02171  0.00735  0.01175  0.01176       0.00650  0.00639   \n",
      "me            0.03091  0.01649  0.02268  0.01070       0.01073  0.00531   \n",
      "of            0.02552  0.00766  0.00844  0.01251       0.01070  0.00697   \n",
      "my            0.03108  0.01506  0.01770  0.01196       0.01116  0.00621   \n",
      "childhood     0.02202  0.00718  0.01005  0.01088       0.01491  0.01271   \n",
      ".             0.00867  0.00368  0.00383  0.00241       0.00835  0.00384   \n",
      "[SEP]         0.00833  0.00344  0.00361  0.00233       0.00819  0.00373   \n",
      "\n",
      "                   is    great      and       it  reminds       me       of  \\\n",
      "[CLS]         0.04323  0.03928  0.05534  0.05027  0.04018  0.02225  0.03726   \n",
      "i             0.01764  0.01439  0.02493  0.02087  0.01611  0.01408  0.02156   \n",
      "think         0.02234  0.01542  0.01881  0.01753  0.02351  0.01462  0.01408   \n",
      "the           0.02732  0.01981  0.01096  0.02408  0.01895  0.00535  0.02959   \n",
      "transformers  0.01625  0.01135  0.01444  0.01018  0.00921  0.00514  0.00964   \n",
      "movie         0.01554  0.01420  0.01210  0.01190  0.01041  0.00400  0.01303   \n",
      "is            0.07248  0.02719  0.01613  0.02149  0.02930  0.00721  0.01716   \n",
      "great         0.02883  0.06653  0.01798  0.01859  0.02703  0.00648  0.01579   \n",
      "and           0.01949  0.01717  0.09580  0.01716  0.02280  0.01297  0.01430   \n",
      "it            0.02470  0.02529  0.02316  0.03104  0.03687  0.00879  0.03093   \n",
      "reminds       0.02969  0.02333  0.01568  0.01769  0.10274  0.01677  0.03605   \n",
      "me            0.01529  0.01798  0.02279  0.01379  0.02588  0.02461  0.02218   \n",
      "of            0.01826  0.01592  0.01594  0.01916  0.04879  0.01020  0.08621   \n",
      "my            0.01580  0.01602  0.01723  0.01263  0.02297  0.01642  0.02757   \n",
      "childhood     0.01344  0.01976  0.01286  0.01081  0.01832  0.00811  0.02637   \n",
      ".             0.00310  0.00240  0.00565  0.00168  0.00278  0.00312  0.00218   \n",
      "[SEP]         0.00296  0.00228  0.00529  0.00160  0.00260  0.00289  0.00202   \n",
      "\n",
      "                   my  childhood        .    [SEP]  \n",
      "[CLS]         0.04265    0.04861  0.15770  0.14962  \n",
      "i             0.01991    0.01320  0.39605  0.34480  \n",
      "think         0.01487    0.01354  0.38709  0.33717  \n",
      "the           0.00814    0.00916  0.37628  0.33013  \n",
      "transformers  0.00708    0.01322  0.35671  0.31642  \n",
      "movie         0.00587    0.01610  0.36527  0.32316  \n",
      "is            0.00889    0.00835  0.37649  0.32611  \n",
      "great         0.00755    0.01148  0.38228  0.33171  \n",
      "and           0.01430    0.01103  0.36540  0.31813  \n",
      "it            0.01115    0.01347  0.37760  0.32787  \n",
      "reminds       0.01538    0.01107  0.35907  0.30707  \n",
      "me            0.02532    0.01906  0.38115  0.33513  \n",
      "of            0.01746    0.01700  0.36578  0.31347  \n",
      "my            0.03681    0.02797  0.38198  0.33142  \n",
      "childhood     0.01760    0.10394  0.37016  0.32088  \n",
      ".             0.00274    0.00374  0.50972  0.43212  \n",
      "[SEP]         0.00257    0.00357  0.51125  0.43335  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(attention.detach().numpy(), index=tokens, columns=tokens)\n",
    "print(df.round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72635522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size: \", vocab_size)\n",
    "# demension of the model\n",
    "d_model = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca42725a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings:  tensor([[-1.1258e+00, -1.1524e+00, -2.5058e-01, -4.3388e-01,  8.4871e-01,\n",
      "          6.9201e-01, -3.1601e-01, -2.1152e+00],\n",
      "        [ 3.2227e-01, -1.2633e+00,  3.4998e-01,  3.0813e-01,  1.1984e-01,\n",
      "          1.2377e+00,  1.1168e+00, -2.4728e-01],\n",
      "        [-1.3527e+00, -1.6959e+00,  5.6665e-01,  7.9351e-01,  5.9884e-01,\n",
      "         -1.5551e+00, -3.4136e-01,  1.8530e+00],\n",
      "        [ 7.5019e-01, -5.8550e-01, -1.7340e-01,  1.8348e-01,  1.3894e+00,\n",
      "          1.5863e+00,  9.4630e-01, -8.4368e-01],\n",
      "        [-6.1358e-01,  3.1593e-02, -4.9268e-01,  2.4841e-01,  4.3970e-01,\n",
      "          1.1241e-01,  6.4079e-01,  4.4116e-01],\n",
      "        [-1.0231e-01,  7.9244e-01, -2.8967e-01,  5.2507e-02,  5.2286e-01,\n",
      "          2.3022e+00, -1.4689e+00, -1.5867e+00],\n",
      "        [-6.7309e-01,  8.7283e-01,  1.0554e+00,  1.7784e-01, -2.3034e-01,\n",
      "         -3.9175e-01,  5.4329e-01, -3.9516e-01],\n",
      "        [-4.4622e-01,  7.4402e-01,  1.5210e+00,  3.4105e+00, -1.5312e+00,\n",
      "         -1.2341e+00,  1.8197e+00, -5.5153e-01],\n",
      "        [-5.6925e-01,  9.1997e-01,  1.1108e+00,  1.2899e+00, -1.4782e+00,\n",
      "          2.5672e+00, -4.7312e-01,  3.3555e-01],\n",
      "        [-1.6293e+00, -5.4974e-01, -4.7983e-01, -4.9968e-01, -1.0670e+00,\n",
      "          1.1149e+00, -1.4067e-01,  8.0575e-01],\n",
      "        [-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
      "         -4.0003e-01,  1.0395e+00,  3.5815e-01],\n",
      "        [-2.4600e-01,  2.3025e+00, -1.8817e+00, -4.9727e-02, -1.0450e+00,\n",
      "         -9.5650e-01,  3.3532e-02,  7.1009e-01],\n",
      "        [ 1.6459e+00, -1.3602e+00,  3.4457e-01,  5.1987e-01, -2.6133e+00,\n",
      "         -1.6965e+00, -2.2824e-01,  2.7995e-01],\n",
      "        [ 2.4693e-01,  7.6887e-02,  3.3801e-01,  4.5440e-01,  4.5694e-01,\n",
      "         -8.6537e-01,  7.8131e-01, -9.2679e-01]])\n",
      "Token Embeddings:  tensor([[-1.1258e+00, -1.1524e+00, -2.5058e-01, -4.3388e-01,  8.4871e-01,\n",
      "          6.9201e-01, -3.1601e-01, -2.1152e+00],\n",
      "        [ 3.2227e-01, -1.2633e+00,  3.4998e-01,  3.0813e-01,  1.1984e-01,\n",
      "          1.2377e+00,  1.1168e+00, -2.4728e-01],\n",
      "        [-1.3527e+00, -1.6959e+00,  5.6665e-01,  7.9351e-01,  5.9884e-01,\n",
      "         -1.5551e+00, -3.4136e-01,  1.8530e+00],\n",
      "        [ 7.5019e-01, -5.8550e-01, -1.7340e-01,  1.8348e-01,  1.3894e+00,\n",
      "          1.5863e+00,  9.4630e-01, -8.4368e-01],\n",
      "        [-6.1358e-01,  3.1593e-02, -4.9268e-01,  2.4841e-01,  4.3970e-01,\n",
      "          1.1241e-01,  6.4079e-01,  4.4116e-01],\n",
      "        [-1.0231e-01,  7.9244e-01, -2.8967e-01,  5.2507e-02,  5.2286e-01,\n",
      "          2.3022e+00, -1.4689e+00, -1.5867e+00],\n",
      "        [-6.7309e-01,  8.7283e-01,  1.0554e+00,  1.7784e-01, -2.3034e-01,\n",
      "         -3.9175e-01,  5.4329e-01, -3.9516e-01],\n",
      "        [-4.4622e-01,  7.4402e-01,  1.5210e+00,  3.4105e+00, -1.5312e+00,\n",
      "         -1.2341e+00,  1.8197e+00, -5.5153e-01],\n",
      "        [-5.6925e-01,  9.1997e-01,  1.1108e+00,  1.2899e+00, -1.4782e+00,\n",
      "          2.5672e+00, -4.7312e-01,  3.3555e-01],\n",
      "        [-1.6293e+00, -5.4974e-01, -4.7983e-01, -4.9968e-01, -1.0670e+00,\n",
      "          1.1149e+00, -1.4067e-01,  8.0575e-01],\n",
      "        [-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
      "         -4.0003e-01,  1.0395e+00,  3.5815e-01],\n",
      "        [-2.4600e-01,  2.3025e+00, -1.8817e+00, -4.9727e-02, -1.0450e+00,\n",
      "         -9.5650e-01,  3.3532e-02,  7.1009e-01],\n",
      "        [ 1.6459e+00, -1.3602e+00,  3.4457e-01,  5.1987e-01, -2.6133e+00,\n",
      "         -1.6965e+00, -2.2824e-01,  2.7995e-01],\n",
      "        [ 2.4693e-01,  7.6887e-02,  3.3801e-01,  4.5440e-01,  4.5694e-01,\n",
      "         -8.6537e-01,  7.8131e-01, -9.2679e-01]])\n",
      "Token embeddings shape:  torch.Size([14, 8])\n"
     ]
    }
   ],
   "source": [
    "# Random embeddings initialization\n",
    "embed = torch.randn(vocab_size, d_model)\n",
    "print(\"Embeddings: \", embed)    \n",
    "# Get embeddings for the tokens\n",
    "token_embed = embed[tok]\n",
    "print(\"Token Embeddings: \", token_embed)\n",
    "print(\"Token embeddings shape: \", token_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6c112",
   "metadata": {},
   "source": [
    "#### Brief lesson on Simple Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5eec48c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ:  tensor([[-0.2188, -2.4351, -0.0729, -0.0340,  0.9625,  0.3492, -0.9215, -0.0562],\n",
      "        [-0.6227, -0.4637,  1.9218, -0.4025,  0.1239,  1.1648,  0.9234,  1.3873],\n",
      "        [-0.8834, -0.4189, -0.8048,  0.5656,  0.6104,  0.4669,  1.9507, -1.0631],\n",
      "        [-0.0773,  0.1164, -0.5940, -1.2439, -0.1021, -1.0335, -0.3126,  0.2458],\n",
      "        [-0.2596,  0.1183,  0.2440,  1.1646,  0.2886,  0.3866, -0.2011, -0.1179],\n",
      "        [ 0.1922, -0.7722, -1.9003,  0.1307, -0.7043,  0.3147,  0.1574,  0.3854],\n",
      "        [ 0.9671, -0.9911,  0.3016, -0.1073,  0.9985, -0.4987,  0.7611,  0.6183],\n",
      "        [ 0.3140,  0.2133, -0.1201,  0.3605, -0.3140, -1.0787,  0.2408, -1.3962]])\n",
      "WK:  tensor([[-0.0661, -0.3584, -1.5616, -0.3546,  1.0811,  0.1315,  1.5735,  0.7814],\n",
      "        [-1.0787, -0.7209,  1.4708,  0.2756,  0.6668, -0.9944, -1.1894, -1.1959],\n",
      "        [-0.5596,  0.5335,  0.4069,  0.3946,  0.1715,  0.8760, -0.2871,  1.0216],\n",
      "        [-0.0744, -1.0922,  0.3920,  0.5945,  0.6623, -1.2063,  0.6074, -0.5472],\n",
      "        [ 1.1711,  0.0975,  0.9634,  0.8403, -1.2537,  0.9868, -0.4947, -1.2830],\n",
      "        [ 0.9552,  1.2836, -0.6659,  0.5651,  0.2877, -0.0334, -1.0619, -0.1144],\n",
      "        [-0.3433,  1.5713,  0.1916,  0.3799, -0.1448,  0.6376, -0.2813, -1.3299],\n",
      "        [-0.1420, -0.5341, -0.5234,  0.8615, -0.8870,  0.8388,  1.1529, -1.7611]])\n",
      "WV:  tensor([[-1.4777, -1.7557,  0.0762, -1.0786,  1.4403, -0.1106,  0.5769, -0.1692],\n",
      "        [-0.0640,  1.0384,  0.9068, -0.4755, -0.8707,  0.1447,  1.9029,  0.3904],\n",
      "        [-0.0394, -0.8015, -0.4955, -0.3615,  0.5851, -1.1560, -0.1434, -0.1947],\n",
      "        [-0.0856,  1.3945,  0.5969, -0.4828, -0.3661, -1.3271,  1.6953,  2.0655],\n",
      "        [-0.2340,  0.7073,  0.5800,  0.2683, -2.0589,  0.5340, -0.5354, -0.8637],\n",
      "        [-0.0235,  1.1717,  0.3987, -0.1987, -1.1559, -0.3167,  0.9403, -1.1470],\n",
      "        [ 0.5588,  0.7918, -0.1847, -0.7318, -0.0807, -0.9801,  0.0605, -0.4890],\n",
      "        [-0.8137,  0.8200, -0.6332,  1.2948,  1.4628, -0.6204,  0.9884, -0.4322]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights for Q, K, V\n",
    "d_k = d_model\n",
    "WQ = torch.randn(d_model, d_k)\n",
    "print(\"WQ: \", WQ)\n",
    "WK = torch.randn(d_model, d_k)\n",
    "print(\"WK: \", WK)\n",
    "WV = torch.randn(d_model, d_k)\n",
    "print(\"WV: \", WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "89a1424e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  tensor([[ 0.1616,  2.7584, -2.6225,  1.2504, -1.2288,  1.5813, -1.1914,  1.5489],\n",
      "        [ 1.5923, -2.4107, -4.8723,  0.4046,  0.6914, -1.3684,  0.0840, -0.5685],\n",
      "        [ 0.5876,  5.9405, -1.3121,  1.2609, -0.9019, -5.0898,  0.3589, -6.1523],\n",
      "        [ 0.9338, -3.6396, -3.4383,  1.3036,  1.0184,  0.7839, -1.1401,  1.5856],\n",
      "        [ 1.1963,  1.1390,  0.3884,  0.0375, -0.3638, -1.2543,  0.0791,  0.4348],\n",
      "        [-1.8314, -0.5894, -2.7677, -0.0492, -2.6213,  4.0686, -0.9983,  3.5586],\n",
      "        [-0.9564,  0.4654,  1.6710, -0.4730,  0.9623,  1.0335,  3.7322,  0.9343],\n",
      "        [-0.2259, -0.6480,  0.7992, -6.0051,  2.6603, -3.3969,  4.3647,  1.8787],\n",
      "        [-1.0043, -0.9724, -5.2729, -2.5415, -2.7000,  0.1688,  3.5596,  0.8470],\n",
      "        [ 1.7697,  3.6894, -2.7729, -0.1645, -3.3649, -1.7774,  0.6910, -0.9403],\n",
      "        [ 1.1554, -0.2853,  3.2374,  0.1971,  0.9336, -0.3298, -0.2700,  1.7390],\n",
      "        [ 0.6291,  1.0469,  7.4745, -3.0106, -0.9123,  0.2812, -1.0461,  3.9802],\n",
      "        [ 0.3619, -2.1743, -0.8362, -3.1000,  1.6978, -3.1184, -2.1108, -3.0956],\n",
      "        [-0.2560, -0.9754,  1.6907, -0.4123,  2.2196,  0.3783,  0.5041,  1.2349]])\n",
      "K:  tensor([[ 3.5538,  3.1787,  1.1945, -1.1133, -1.2588,  0.1405, -4.0970,  3.4570],\n",
      "        [ 2.0969,  4.1326, -2.4634,  0.8702,  0.0336,  1.8151,  0.1236,  0.6066],\n",
      "        [ 0.6125, -2.3209,  0.7367,  1.7987, -4.7627,  3.0272,  3.7954, -2.2838],\n",
      "        [ 3.6026,  3.9695, -1.1261,  1.3099, -0.1617,  1.5215, -1.5726, -0.7279],\n",
      "        [ 0.6034,  0.6213,  1.1422,  1.2361, -1.5652,  0.3654, -0.7192, -3.3626],\n",
      "        [ 2.8513,  0.7990,  0.7477, -0.0130,  2.0297, -2.9469, -5.1079,  2.4611],\n",
      "        [-2.2752,  0.5202,  3.1838,  0.4525,  0.6010, -0.4457, -2.3707, -0.2752],\n",
      "        [-5.3964, -1.8696,  3.7310,  1.2232,  4.3235, -4.3523,  0.9682, -0.8936],\n",
      "        [-0.8363,  0.9530, -0.2001,  1.9786,  3.4054, -3.1371, -3.0003,  0.5251],\n",
      "        [ 0.7558,  1.9456, -0.8744,  0.3141, -1.5771,  0.0109, -1.7634, -0.8231],\n",
      "        [-0.0698,  0.1005,  1.9047,  1.0771, -1.4247,  0.3764, -0.5940, -4.7991],\n",
      "        [-3.6604, -4.1775,  2.2505, -0.8443,  1.3138, -4.2927, -0.2738, -4.6861],\n",
      "        [-3.5155, -2.9338, -5.8049, -3.5139,  3.8486, -1.1893,  7.9056,  6.3380],\n",
      "        [-0.7504,  0.1966,  1.6944, -0.2695,  0.5642, -0.0954, -0.1193,  0.3036]])\n",
      "V:  tensor([[ 3.1143e+00, -1.9782e-01,  9.0030e-01, -3.5486e-01, -6.2220e+00,\n",
      "          2.6793e+00, -5.4553e+00, -1.5648e+00],\n",
      "        [ 3.3266e-01,  4.8785e-01, -5.9730e-01, -1.3734e+00, -4.7300e-01,\n",
      "         -2.3010e+00, -8.2309e-01, -1.9417e+00],\n",
      "        [ 2.1508e-01,  1.1168e+00, -2.8310e+00,  4.7961e+00,  2.8723e+00,\n",
      "         -1.8068e+00, -2.7155e+00,  1.7279e+00],\n",
      "        [-2.2697e-01,  1.3686e+00,  1.5194e+00, -2.2839e+00, -4.5831e+00,\n",
      "         -3.7513e-01, -3.7433e-01, -3.0602e+00],\n",
      "        [ 7.9642e-01,  3.1632e+00,  2.7653e-01,  9.0286e-01, -1.7321e+00,\n",
      "         -3.9020e-01,  5.4302e-01, -2.8748e-01],\n",
      "        [ 4.0126e-01,  1.9111e+00,  3.3828e+00, -1.4838e+00, -6.9664e+00,\n",
      "          2.3654e+00,  1.8071e+00, -1.1966e+00],\n",
      "        [ 1.5702e+00,  9.7447e-01,  1.8349e-01, -1.0496e+00, -8.7187e-01,\n",
      "         -1.5415e+00,  8.2004e-01,  1.1698e+00],\n",
      "        [ 2.1130e+00,  3.5525e+00,  5.5569e-01, -4.2804e+00,  1.9765e+00,\n",
      "         -7.9953e+00,  5.9463e+00,  9.2007e+00],\n",
      "        [ 3.7628e-01,  4.7262e+00,  1.0514e+00, -9.7393e-01, -8.3833e-01,\n",
      "         -4.1466e+00,  6.9580e+00,  1.3218e+00],\n",
      "        [ 1.9937e+00,  3.0784e+00, -1.3417e+00,  3.0719e+00,  1.3214e-01,\n",
      "          3.3511e-02, -3.5676e-01, -1.5143e+00],\n",
      "        [ 2.2873e-01,  2.7939e+00,  9.4199e-01,  8.5062e-02, -2.0544e+00,\n",
      "          4.1301e-01,  9.6518e-01, -4.8222e-01],\n",
      "        [ 2.3208e-03,  3.0106e+00,  1.5287e+00,  6.7928e-01,  8.5121e-01,\n",
      "          1.8731e+00,  4.7889e+00,  2.8805e+00],\n",
      "        [-2.1072e+00, -7.6405e+00, -3.2958e+00, -1.3386e+00,  1.1336e+01,\n",
      "         -2.2755e+00, -7.3998e-01,  4.3907e+00],\n",
      "        [ 6.8218e-01, -8.2299e-01,  5.5483e-01, -2.1216e+00, -1.0391e+00,\n",
      "         -6.8260e-01, -9.1649e-01,  1.4774e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Dot Product Attentioin \n",
    "Q = token_embed @ WQ\n",
    "print(\"Q: \", Q)\n",
    "K = token_embed @ WK\n",
    "print(\"K: \", K)\n",
    "V = token_embed @ WV\n",
    "print(\"V: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74233f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ooutput:  tensor([[ 4.5973e-01,  6.4362e-01, -4.0198e-02, -1.4335e+00, -1.7924e+00,\n",
      "         -1.4687e+00, -1.0978e+00, -2.1365e+00],\n",
      "        [-2.1062e+00, -7.6363e+00, -3.2941e+00, -1.3382e+00,  1.1331e+01,\n",
      "         -2.2750e+00, -7.3850e-01,  4.3888e+00],\n",
      "        [ 2.6113e-01,  2.5553e+00,  8.9947e-01, -2.2560e-01, -2.3838e+00,\n",
      "          7.4905e-02,  7.0550e-01, -9.4588e-01],\n",
      "        [-2.1039e+00, -7.6271e+00, -3.2885e+00, -1.3382e+00,  1.1315e+01,\n",
      "         -2.2722e+00, -7.3533e-01,  4.3843e+00],\n",
      "        [ 1.5734e+00,  9.6790e-01,  1.3899e+00, -8.3567e-01, -5.0847e+00,\n",
      "          1.3496e+00, -1.7171e+00, -1.3990e+00],\n",
      "        [-2.0462e+00, -7.4694e+00, -3.2554e+00, -1.2749e+00,  1.1102e+01,\n",
      "         -2.2494e+00, -7.7797e-01,  4.2973e+00],\n",
      "        [-2.1019e+00, -7.6255e+00, -3.2914e+00, -1.3395e+00,  1.1322e+01,\n",
      "         -2.2810e+00, -7.3379e-01,  4.3944e+00],\n",
      "        [-2.1072e+00, -7.6405e+00, -3.2958e+00, -1.3386e+00,  1.1336e+01,\n",
      "         -2.2755e+00, -7.3998e-01,  4.3907e+00],\n",
      "        [-2.1072e+00, -7.6405e+00, -3.2958e+00, -1.3386e+00,  1.1336e+01,\n",
      "         -2.2755e+00, -7.3998e-01,  4.3907e+00],\n",
      "        [ 2.4996e-01,  1.0047e+00,  2.8328e-01, -1.4635e+00, -2.2881e+00,\n",
      "         -1.2448e+00, -6.9871e-01, -2.3692e+00],\n",
      "        [ 1.5029e+00,  1.4076e+00,  1.9573e+00, -1.4616e+00, -4.9495e+00,\n",
      "          7.2175e-01,  1.0886e-01,  2.5365e-01],\n",
      "        [ 3.0971e+00, -1.8520e-01,  9.0398e-01, -3.6266e-01, -6.1943e+00,\n",
      "          2.6547e+00, -5.3983e+00, -1.5479e+00],\n",
      "        [ 3.1975e-03,  3.0108e+00,  1.5286e+00,  6.7698e-01,  8.5021e-01,\n",
      "          1.8692e+00,  4.7889e+00,  2.8820e+00],\n",
      "        [ 4.0086e-01, -9.0923e-01, -9.2830e-01, -2.9882e+00,  5.4973e+00,\n",
      "         -5.4369e+00,  3.1722e+00,  6.9828e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Attention Scores, wee used the formula for the Attention as discussed in the slides\n",
    "scores = (Q @ K.T) / (d_k ** 0.5)\n",
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "output = attention_weights @ V\n",
    "print(\"Ooutput: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0790bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    i    think      the  transformers    movie       is  \\\n",
      "i             0.08135  0.69940  0.00006       0.20771  0.00013  0.00071   \n",
      "think         0.00000  0.00008  0.00001       0.00003  0.00000  0.00007   \n",
      "the           0.00001  0.01221  0.00000       0.20576  0.14167  0.00005   \n",
      "transformers  0.00001  0.00005  0.00004       0.00002  0.00000  0.00081   \n",
      "movie         0.43147  0.06218  0.00298       0.12072  0.01643  0.25209   \n",
      "is            0.00407  0.00622  0.00941       0.00017  0.00000  0.00000   \n",
      "great         0.00000  0.00001  0.00034       0.00000  0.00000  0.00000   \n",
      "and           0.00000  0.00000  0.00000       0.00000  0.00000  0.00000   \n",
      "it            0.00000  0.00000  0.00000       0.00000  0.00000  0.00000   \n",
      "reminds       0.02035  0.49522  0.00160       0.42160  0.00289  0.00013   \n",
      "me            0.29727  0.00038  0.00035       0.00171  0.00123  0.46527   \n",
      "of            0.99135  0.00000  0.00000       0.00000  0.00000  0.00305   \n",
      "my            0.00000  0.00000  0.00000       0.00000  0.00000  0.00016   \n",
      "childhood.    0.00084  0.00012  0.00011       0.00008  0.00014  0.00629   \n",
      "\n",
      "                great      and       it  reminds       me       of       my  \\\n",
      "i             0.00003  0.00000  0.00028  0.01022  0.00001  0.00000  0.00005   \n",
      "think         0.00000  0.00000  0.00012  0.00003  0.00000  0.00006  0.99959   \n",
      "the           0.00004  0.00028  0.00975  0.02648  0.58470  0.01905  0.00000   \n",
      "transformers  0.00000  0.00000  0.00035  0.00002  0.00000  0.00001  0.99868   \n",
      "movie         0.01018  0.00395  0.03649  0.03762  0.00873  0.00200  0.00140   \n",
      "is            0.00002  0.00000  0.00000  0.00053  0.00000  0.00000  0.97952   \n",
      "great         0.00001  0.00102  0.00000  0.00000  0.00000  0.00000  0.99856   \n",
      "and           0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  1.00000   \n",
      "it            0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  1.00000   \n",
      "reminds       0.00000  0.00000  0.00002  0.05752  0.00064  0.00000  0.00000   \n",
      "me            0.06606  0.12420  0.01579  0.00058  0.00100  0.00204  0.00010   \n",
      "of            0.00526  0.00005  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
      "my            0.00000  0.00037  0.00007  0.00000  0.00001  0.99939  0.00000   \n",
      "childhood.    0.00992  0.56666  0.00635  0.00007  0.00017  0.00871  0.39190   \n",
      "\n",
      "              childhood.  \n",
      "i                0.00004  \n",
      "think            0.00000  \n",
      "the              0.00001  \n",
      "transformers     0.00001  \n",
      "movie            0.01376  \n",
      "is               0.00005  \n",
      "great            0.00005  \n",
      "and              0.00000  \n",
      "it               0.00000  \n",
      "reminds          0.00002  \n",
      "me               0.02402  \n",
      "of               0.00028  \n",
      "my               0.00000  \n",
      "childhood.       0.00863  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(attention_weights.detach().numpy(), columns=[inverse_vocab[i] for i in range(len(vocab))], index=[inverse_vocab[i] for i in range(len(vocab))])\n",
    "# pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "print(df.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2822f",
   "metadata": {},
   "source": [
    "#### Brief lesson on Simple Postional \n",
    "\n",
    "Transformers process all words in parallel, (part of the reason for the rise in GPUs). They do not know whta word comes first nor the position each word is in. Adding position to  each token embedding tells the model where the word sits in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1cacf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings with Position encoding:  tensor([[-1.1258,  0.8476, -0.2506,  1.5661,  0.8487,  2.6920, -0.3160, -0.1152],\n",
      "        [ 2.0052,  0.7267,  0.3700,  2.3081,  0.1200,  3.2377,  1.1168,  1.7527],\n",
      "        [ 0.4659,  0.2642,  0.6066,  2.7935,  0.5992,  0.4449, -0.3414,  3.8530],\n",
      "        [ 1.0324,  1.3252, -0.1134,  2.1835,  1.3900,  3.5863,  0.9463,  1.1563],\n",
      "        [-2.1272,  1.8737, -0.4127,  2.2484,  0.4405,  2.1124,  0.6408,  2.4412],\n",
      "        [-2.0202,  2.5476, -0.1897,  2.0525,  0.5239,  4.3022, -1.4689,  0.4133],\n",
      "        [-1.2319,  2.5235,  1.1753,  2.1778, -0.2291,  1.6082,  0.5433,  1.6048],\n",
      "        [ 0.8678,  2.2737,  1.6609,  5.4105, -1.5298,  0.7659,  1.8197,  1.4485],\n",
      "        [ 1.4095,  2.3134,  1.2706,  3.2898, -1.4766,  4.5672, -0.4731,  2.3356],\n",
      "        [-0.8051,  0.6935, -0.3001,  1.5002, -1.0652,  3.1149, -0.1407,  2.8058],\n",
      "        [-1.1814,  1.7677, -0.6386,  2.0008,  0.8439,  1.6000,  1.0395,  2.3582],\n",
      "        [-2.2460,  3.2097, -1.6621,  1.9502, -1.0428,  1.0435,  0.0336,  2.7101],\n",
      "        [ 0.5727, -0.6355,  0.5840,  2.5197, -2.6109,  0.3035, -0.2282,  2.2800],\n",
      "        [ 1.0873,  0.6119,  0.5973,  2.4542,  0.4595,  1.1346,  0.7813,  1.0732]])\n"
     ]
    }
   ],
   "source": [
    "def pose_encoding(T, d_model):\n",
    "    pe = torch.zeros(T, d_model)\n",
    "    for pos in range (T):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "            pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "\n",
    "    return pe\n",
    "\n",
    "token_embed = token_embed + pose_encoding(token_embed.size(0), d_model)\n",
    "print(\"Token embeddings with Position encoding: \", token_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fec1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".trenv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
